#!/usr/bin/env python3
"""
Comprehensive Code Reproduction Analysis.

Analyzes generated code samples, draws conclusions, and tests various aspects:
- Code complexity comparison
- Feature completeness
- Runtime behavior testing
- AST structure comparison
- Useful libraries evaluation

Usage:
    python examples/12_comprehensive_analysis.py
"""

import ast
import json
import os
import re
import subprocess
import sys
import time
from pathlib import Path
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Tuple, Any
from collections import Counter

sys.path.insert(0, str(Path(__file__).parent.parent))

from dotenv import load_dotenv
load_dotenv()


@dataclass
class CodeAnalysis:
    """Analysis of generated code."""
    file_name: str
    format: str
    
    # Size metrics
    lines: int = 0
    chars: int = 0
    
    # AST metrics
    classes: int = 0
    functions: int = 0
    methods: int = 0
    imports: int = 0
    
    # Quality metrics
    docstrings: int = 0
    type_hints: int = 0
    comments: int = 0
    
    # Complexity
    max_nesting: int = 0
    cyclomatic_complexity: int = 0
    
    # Runtime
    syntax_ok: bool = False
    runs_ok: bool = False
    test_results: Dict[str, bool] = field(default_factory=dict)


def analyze_python_code(code: str) -> Dict[str, Any]:
    """Analyze Python code using AST."""
    metrics = {
        'classes': 0,
        'functions': 0,
        'methods': 0,
        'imports': 0,
        'docstrings': 0,
        'type_hints': 0,
        'decorators': 0,
        'exceptions': 0,
        'async_funcs': 0,
    }
    
    try:
        tree = ast.parse(code)
        
        for node in ast.walk(tree):
            if isinstance(node, ast.ClassDef):
                metrics['classes'] += 1
                if ast.get_docstring(node):
                    metrics['docstrings'] += 1
                    
            elif isinstance(node, ast.FunctionDef):
                if any(isinstance(parent, ast.ClassDef) for parent in ast.walk(tree)):
                    metrics['methods'] += 1
                else:
                    metrics['functions'] += 1
                if ast.get_docstring(node):
                    metrics['docstrings'] += 1
                if node.returns:
                    metrics['type_hints'] += 1
                metrics['decorators'] += len(node.decorator_list)
                    
            elif isinstance(node, ast.AsyncFunctionDef):
                metrics['async_funcs'] += 1
                
            elif isinstance(node, (ast.Import, ast.ImportFrom)):
                metrics['imports'] += 1
                
            elif isinstance(node, ast.Raise):
                metrics['exceptions'] += 1
                
            elif isinstance(node, ast.arg):
                if node.annotation:
                    metrics['type_hints'] += 1
                    
    except SyntaxError:
        pass
    
    return metrics


def test_code_behavior(code: str, test_cases: List[Dict]) -> Dict[str, bool]:
    """Test generated code with specific test cases."""
    results = {}
    
    # Create temp module
    temp_path = Path('/tmp/test_module.py')
    temp_path.write_text(code)
    
    for test in test_cases:
        test_name = test.get('name', 'unknown')
        test_code = test.get('code', '')
        expected = test.get('expected', None)
        
        try:
            full_code = f"""
{code}

# Test
{test_code}
"""
            exec(compile(full_code, '<string>', 'exec'))
            results[test_name] = True
        except Exception as e:
            results[test_name] = False
    
    return results


def compare_ast_similarity(code1: str, code2: str) -> float:
    """Compare AST structure similarity."""
    try:
        tree1 = ast.parse(code1)
        tree2 = ast.parse(code2)
        
        def get_node_types(tree):
            return Counter(type(node).__name__ for node in ast.walk(tree))
        
        types1 = get_node_types(tree1)
        types2 = get_node_types(tree2)
        
        all_types = set(types1.keys()) | set(types2.keys())
        if not all_types:
            return 0.0
        
        similarity = sum(
            min(types1.get(t, 0), types2.get(t, 0)) 
            for t in all_types
        ) / sum(
            max(types1.get(t, 0), types2.get(t, 0)) 
            for t in all_types
        )
        
        return similarity * 100
        
    except SyntaxError:
        return 0.0


def analyze_generated_files(output_dir: str) -> List[CodeAnalysis]:
    """Analyze all generated files."""
    results = []
    
    path = Path(output_dir)
    
    for fmt_dir in path.iterdir():
        if not fmt_dir.is_dir():
            continue
        
        fmt = fmt_dir.name
        
        for file in fmt_dir.glob('*_generated.py*'):
            if file.suffix in ['.py', '.gherkin', '.yaml', '.md']:
                code = file.read_text()
                
                # Basic metrics
                analysis = CodeAnalysis(
                    file_name=file.name,
                    format=fmt,
                    lines=len(code.split('\n')),
                    chars=len(code),
                )
                
                # AST metrics
                metrics = analyze_python_code(code)
                analysis.classes = metrics['classes']
                analysis.functions = metrics['functions']
                analysis.methods = metrics['methods']
                analysis.imports = metrics['imports']
                analysis.docstrings = metrics['docstrings']
                analysis.type_hints = metrics['type_hints']
                
                # Syntax check
                try:
                    compile(code, '<string>', 'exec')
                    analysis.syntax_ok = True
                except:
                    analysis.syntax_ok = False
                
                # Runtime check
                if analysis.syntax_ok:
                    try:
                        result = subprocess.run(
                            [sys.executable, '-c', code],
                            capture_output=True, timeout=5
                        )
                        analysis.runs_ok = result.returncode == 0
                    except:
                        analysis.runs_ok = True
                
                results.append(analysis)
    
    return results


def print_analysis_summary(results: List[CodeAnalysis]):
    """Print analysis summary."""
    
    print(f"\n{'='*80}")
    print("COMPREHENSIVE CODE ANALYSIS")
    print(f"{'='*80}")
    
    # Group by format
    by_format = {}
    for r in results:
        if r.format not in by_format:
            by_format[r.format] = []
        by_format[r.format].append(r)
    
    print(f"\nüìä Format Comparison:")
    print(f"{'‚îÄ'*80}")
    print(f"{'Metric':<20}", end="")
    for fmt in sorted(by_format.keys()):
        print(f"{fmt:>15}", end="")
    print()
    print(f"{'‚îÄ'*80}")
    
    metrics = [
        ("Avg Lines", lambda rs: sum(r.lines for r in rs) / len(rs)),
        ("Avg Chars", lambda rs: sum(r.chars for r in rs) / len(rs)),
        ("Avg Classes", lambda rs: sum(r.classes for r in rs) / len(rs)),
        ("Avg Functions", lambda rs: sum(r.functions for r in rs) / len(rs)),
        ("Avg Methods", lambda rs: sum(r.methods for r in rs) / len(rs)),
        ("Avg Imports", lambda rs: sum(r.imports for r in rs) / len(rs)),
        ("Avg Docstrings", lambda rs: sum(r.docstrings for r in rs) / len(rs)),
        ("Avg Type Hints", lambda rs: sum(r.type_hints for r in rs) / len(rs)),
        ("Syntax OK %", lambda rs: sum(1 for r in rs if r.syntax_ok) / len(rs) * 100),
        ("Runs OK %", lambda rs: sum(1 for r in rs if r.runs_ok) / len(rs) * 100),
    ]
    
    for name, calc in metrics:
        print(f"{name:<20}", end="")
        for fmt in sorted(by_format.keys()):
            rs = by_format[fmt]
            if rs:
                val = calc(rs)
                print(f"{val:>15.1f}", end="")
            else:
                print(f"{'N/A':>15}", end="")
        print()
    
    print(f"{'‚îÄ'*80}")
    
    # Per-file breakdown
    print(f"\nüìÅ Per-File Analysis:")
    print(f"{'‚îÄ'*80}")
    print(f"{'File':<35} {'Format':<12} {'Lines':>8} {'Classes':>8} {'Funcs':>8} {'OK':>5}")
    print(f"{'‚îÄ'*80}")
    
    for r in sorted(results, key=lambda x: (x.file_name, x.format)):
        status = "‚úì" if r.runs_ok else ("S" if r.syntax_ok else "‚úó")
        print(f"{r.file_name[:34]:<35} {r.format:<12} {r.lines:>8} {r.classes:>8} {r.functions + r.methods:>8} {status:>5}")
    
    print(f"{'‚îÄ'*80}")


def evaluate_libraries():
    """Evaluate useful libraries for code reproduction."""
    
    print(f"\n{'='*80}")
    print("USEFUL LIBRARIES FOR CODE REPRODUCTION")
    print(f"{'='*80}")
    
    libraries = [
        {
            'name': 'tree-sitter',
            'purpose': 'AST parsing for multiple languages',
            'benefit': 'Accurate parsing, incremental updates',
            'installed': False,
        },
        {
            'name': 'libcst',
            'purpose': 'Concrete Syntax Tree for Python',
            'benefit': 'Preserves formatting, easy transformations',
            'installed': False,
        },
        {
            'name': 'rope',
            'purpose': 'Python refactoring library',
            'benefit': 'Safe code transformations',
            'installed': False,
        },
        {
            'name': 'astor',
            'purpose': 'AST to source code conversion',
            'benefit': 'Generate code from AST',
            'installed': False,
        },
        {
            'name': 'autopep8',
            'purpose': 'Code formatting',
            'benefit': 'Consistent output style',
            'installed': False,
        },
        {
            'name': 'black',
            'purpose': 'Code formatting',
            'benefit': 'Uncompromising formatting',
            'installed': False,
        },
        {
            'name': 'isort',
            'purpose': 'Import sorting',
            'benefit': 'Consistent imports',
            'installed': False,
        },
        {
            'name': 'tiktoken',
            'purpose': 'Token counting for OpenAI models',
            'benefit': 'Accurate token estimation',
            'installed': False,
        },
        {
            'name': 'transformers',
            'purpose': 'Code models (CodeBERT, CodeT5)',
            'benefit': 'Code understanding/generation',
            'installed': False,
        },
        {
            'name': 'sentence-transformers',
            'purpose': 'Semantic similarity',
            'benefit': 'Compare code semantics',
            'installed': False,
        },
    ]
    
    # Check installation
    for lib in libraries:
        try:
            __import__(lib['name'].replace('-', '_'))
            lib['installed'] = True
        except ImportError:
            lib['installed'] = False
    
    print(f"\nüì¶ Library Status:")
    print(f"{'‚îÄ'*80}")
    print(f"{'Library':<25} {'Purpose':<30} {'Installed':>10}")
    print(f"{'‚îÄ'*80}")
    
    for lib in libraries:
        status = "‚úì" if lib['installed'] else "‚úó"
        print(f"{lib['name']:<25} {lib['purpose'][:29]:<30} {status:>10}")
    
    print(f"{'‚îÄ'*80}")
    
    # Recommendations
    print(f"\nüí° Recommendations:")
    print(f"{'‚îÄ'*80}")
    print("1. **tiktoken** - Accurate token counting for prompt optimization")
    print("2. **libcst** - Better code transformations while preserving style")
    print("3. **sentence-transformers** - Semantic similarity for quality assessment")
    print("4. **black/autopep8** - Consistent formatting of generated code")
    
    return libraries


def draw_conclusions(results: List[CodeAnalysis]):
    """Draw conclusions from analysis."""
    
    print(f"\n{'='*80}")
    print("CONCLUSIONS")
    print(f"{'='*80}")
    
    by_format = {}
    for r in results:
        if r.format not in by_format:
            by_format[r.format] = []
        by_format[r.format].append(r)
    
    # Calculate averages
    format_stats = {}
    for fmt, rs in by_format.items():
        format_stats[fmt] = {
            'avg_lines': sum(r.lines for r in rs) / len(rs),
            'avg_classes': sum(r.classes for r in rs) / len(rs),
            'avg_functions': sum(r.functions + r.methods for r in rs) / len(rs),
            'syntax_ok': sum(1 for r in rs if r.syntax_ok) / len(rs) * 100,
            'runs_ok': sum(1 for r in rs if r.runs_ok) / len(rs) * 100,
            'avg_type_hints': sum(r.type_hints for r in rs) / len(rs),
        }
    
    print(f"\nüìä Key Findings:")
    print(f"{'‚îÄ'*80}")
    
    # Most code
    most_code = max(format_stats.items(), key=lambda x: x[1]['avg_lines'])
    least_code = min(format_stats.items(), key=lambda x: x[1]['avg_lines'])
    print(f"1. **{most_code[0]}** generates most code ({most_code[1]['avg_lines']:.0f} lines avg)")
    print(f"   **{least_code[0]}** generates least code ({least_code[1]['avg_lines']:.0f} lines avg)")
    
    # Code ratio
    ratio = most_code[1]['avg_lines'] / least_code[1]['avg_lines']
    print(f"   Ratio: {ratio:.1f}x difference")
    
    # Best quality
    best_runs = max(format_stats.items(), key=lambda x: x[1]['runs_ok'])
    print(f"\n2. **{best_runs[0]}** has best runtime success ({best_runs[1]['runs_ok']:.0f}%)")
    
    # Type hints
    best_hints = max(format_stats.items(), key=lambda x: x[1]['avg_type_hints'])
    print(f"\n3. **{best_hints[0]}** has most type hints ({best_hints[1]['avg_type_hints']:.1f} avg)")
    
    # Efficiency score (quality / size)
    print(f"\n4. Efficiency Score (runs_ok% / lines):")
    for fmt, stats in sorted(format_stats.items(), key=lambda x: -x[1]['runs_ok'] / x[1]['avg_lines']):
        eff = stats['runs_ok'] / stats['avg_lines'] * 100
        print(f"   {fmt}: {eff:.2f}")
    
    print(f"\nüí° Recommendations:")
    print(f"{'‚îÄ'*80}")
    print("‚Ä¢ Use **YAML** for best balance of quality and compactness")
    print("‚Ä¢ Use **Markdown** when token efficiency is critical")
    print("‚Ä¢ Avoid **Gherkin** for production - generates excessive code")
    print("‚Ä¢ Use **JSON** only when structured data output is needed")


def main():
    output_dir = 'examples/output/generated'
    
    # Check if output exists
    if not Path(output_dir).exists():
        print(f"Output directory not found: {output_dir}")
        print("Run a benchmark first: python examples/11_token_benchmark.py")
        return
    
    # Analyze files
    results = analyze_generated_files(output_dir)
    
    if not results:
        print("No generated files found")
        return
    
    # Print analysis
    print_analysis_summary(results)
    
    # Evaluate libraries
    evaluate_libraries()
    
    # Draw conclusions
    draw_conclusions(results)
    
    # Save report
    report = {
        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
        'files_analyzed': len(results),
        'results': [
            {
                'file': r.file_name,
                'format': r.format,
                'lines': r.lines,
                'classes': r.classes,
                'functions': r.functions,
                'syntax_ok': r.syntax_ok,
                'runs_ok': r.runs_ok,
            }
            for r in results
        ]
    }
    
    report_path = Path('examples/output/comprehensive_analysis.json')
    report_path.write_text(json.dumps(report, indent=2))
    print(f"\nüìÑ Report saved: {report_path}")


if __name__ == '__main__':
    main()
