meta:
  legend:
    p: path
    l: lines
    i: imports
    e: exports
    c: classes
    f: functions
    n: name
    d: docstring
    b: bases
    m: methods
    props: properties
    sig: signature (without self)
    ret: return_type
    async: is_async
    lang: language
defaults:
  lang: python
modules:
- p: llm_profiler.py
  l: 491
  i:
  - json
  - os
  - time
  - hashlib
  - datetime
  - dataclasses.{asdict,dataclass,field}
  - difflib.SequenceMatcher
  - pathlib.Path
  - typing.{Any,Dict,List,Optional,Tuple}
  e:
  - LLMProfile
  - ProfileTestResult
  - LLMProfiler
  - AdaptiveChunker
  - load_profiles
  - save_profile
  - get_profile
  - get_or_create_profile
  - profile_llm
  - get_adaptive_chunker
  c:
  - n: LLMProfile
    d: Profile of LLM capabilities for code reproduction.
    props:
    - 'provider: str'
    - 'model: str'
    - 'profile_id: str'
    - 'created_at: str'
    - 'effective_context: int'
    - 'max_output: int'
    - 'optimal_chunk_size: int'
    - 'syntax_accuracy: float'
    - 'semantic_accuracy: float'
    - 'type_hint_accuracy: float'
    - 'docstring_accuracy: float'
    - 'output_consistency: float'
    - 'temperature_sensitivity: float'
    - 'hallucination_rate: float'
    - 'omission_rate: float'
    m:
    - n: __post_init__
      sig: ''
      d: creates init
  - n: ProfileTestResult
    d: Result of a single profile test.
    props:
    - 'test_name: str'
    - 'original_code: str'
    - 'reproduced_code: str'
    - 'syntax_ok: bool'
    - 'similarity: float'
    - 'time_seconds: float'
    - 'tokens_in: int'
    - 'tokens_out: int'
    - 'error: str'
  - n: LLMProfiler
    d: Profile LLM capabilities for code reproduction.
    m:
    - n: __init__
      sig: client, verbose:bool=True
      d: Initialize profiler.
    - n: run_profile
      sig: quick:bool=False
      ret: LLMProfile
      d: Run full profiling suite.
    - n: _test_reproduction
      sig: name:str, code:str
      ret: ProfileTestResult
      d: Test reproduction of a code snippet.
    - n: _code_to_spec
      sig: code:str
      ret: str
      d: Convert code to simple YAML spec.
    - n: _extract_code
      sig: response:str
      ret: str
      d: Extract code from LLM response.
    - n: _check_syntax
      sig: code:str
      ret: bool
      d: Check if code has valid Python syntax.
    - n: _calculate_similarity
      sig: original:str, reproduced:str
      ret: float
      d: Calculate code similarity.
    - n: _calculate_metrics
      sig: profile:LLMProfile, results:List[ProfileTestResult]
      ret: LLMProfile
      d: Calculate aggregate metrics from test results.
    - n: _test_consistency
      sig: profile:LLMProfile
      ret: LLMProfile
      d: Test output consistency by running same prompt twi
  - n: AdaptiveChunker
    d: Adaptive chunking based on LLM profile.
    m:
    - n: __init__
      sig: profile:Optional[LLMProfile]=None
      d: Initialize chunker.
    - n: get_optimal_settings
      sig: ''
      ret: Dict[str,Any]
      d: optimal settings for the profiled model.
    - n: chunk_spec
      sig: spec:str, format:str='yaml'
      ret: List[Dict[str,Any]]
      d: Chunk specification based on profile.
    - n: recommend_format
      sig: spec_size_tokens:int
      ret: str
      d: Recommend best format based on spec size and model
    - n: estimate_chunks_needed
      sig: spec_size_tokens:int
      ret: int
      d: Estimate number of chunks needed.
  f:
  - n: _get_profiles_path
    sig: ''
    ret: Path
    d: path to profiles storage.
  - n: load_profiles
    sig: ''
    ret: Dict[str,LLMProfile]
    d: Load all saved profiles.
  - n: save_profile
    sig: profile:LLMProfile
    ret: None
    d: Save a profile to storage.
  - n: get_profile
    sig: provider:str, model:str
    ret: Optional[LLMProfile]
    d: profile for a specific model.
  - n: get_or_create_profile
    sig: provider:str, model:str
    ret: LLMProfile
    d: existing profile or create default one.
  - n: _create_default_profile
    sig: provider:str, model:str
    ret: LLMProfile
    d: default profile based on model characteristics.
  - n: profile_llm
    sig: client, quick:bool=False
    ret: LLMProfile
    d: Profile an LLM client.
  - n: get_adaptive_chunker
    sig: provider:str, model:str
    ret: AdaptiveChunker
    d: adaptive chunker for a model.
  const:
  - n: PROFILE_TEST_CASES
- p: config.py
  l: 168
  i:
  - os
  - json
  - pathlib.Path
  - typing.{Any,Dict,Optional}
  e:
  - Config
  - load_env
  - get_api_key
  - get_model
  c:
  - n: Config
    d: Configuration manager for Code2Logic.
    props:
    - DEFAULT_MODELS
    - API_KEY_VARS
    - MODEL_VARS
    m:
    - n: __init__
      sig: env_file:str=None
      d: Initialize configuration.
    - n: _load_env_file
      sig: env_file:str=None
      d: Load environment variables from .env file.
    - n: _parse_env_file
      sig: path:Path
      d: Parse .env file and set environment variables.
    - n: _load_config_file
      sig: ''
      d: Load configuration from JSON file.
    - n: get_api_key
      sig: provider:str
      ret: Optional[str]
      d: API key for a provider.
    - n: get_model
      sig: provider:str
      ret: str
      d: model for a provider.
    - n: get_ollama_host
      sig: ''
      ret: str
      d: Ollama host URL.
    - n: get_default_provider
      sig: ''
      ret: str
      d: default LLM provider.
    - n: is_verbose
      sig: ''
      ret: bool
      d: Check if verbose mode is enabled.
    - n: get_cache_dir
      sig: ''
      ret: Path
      d: cache directory path.
    - n: list_configured_providers
      sig: ''
      ret: Dict[str,bool]
      d: List all providers and their configuration status.
    - n: to_dict
      sig: ''
      ret: Dict[str,Any]
      d: Export configuration as dictionary.
  f:
  - n: load_env
    sig: ''
    d: Load environment variables from .env file.
  - n: get_api_key
    sig: provider:str
    ret: Optional[str]
    d: Convenience function to get API key.
  - n: get_model
    sig: provider:str
    ret: str
    d: Convenience function to get model.
  const:
  - n: SHELL_COMMANDS
- p: file_formats.py
  l: 279
  i:
  - re
  - json
  - pathlib.Path
  - typing.{Any,Dict,List}
  e:
  - generate_file_csv
  - generate_file_json
  - generate_file_yaml
  f:
  - n: generate_file_csv
    sig: file_path:Path
    ret: str
    d: Generate detailed CSV specification for a single f
  - n: generate_file_json
    sig: file_path:Path
    ret: str
    d: Generate detailed JSON specification for a single
  - n: generate_file_yaml
    sig: file_path:Path
    ret: str
    d: Generate detailed YAML specification for a single
  - n: _parse_file_elements
    sig: content:str
    ret: Dict[str,Any]
    d: Parse file content to extract code elements.
- p: project_reproducer.py
  l: 322
  i:
  - os
  - json
  - hashlib
  - datetime
  - concurrent.futures.{ThreadPoolExecutor,as_completed}
  - dataclasses.{asdict,dataclass,field}
  - pathlib.Path
  - typing.{Any,Dict,List,Optional,Set}
  e:
  - FileResult
  - ProjectResult
  - ProjectReproducer
  - reproduce_project
  c:
  - n: FileResult
    d: Result for a single file reproduction.
    props:
    - 'file_path: str'
    - 'language: str'
    - 'source_chars: int'
    - 'logic_chars: int'
    - 'generated_chars: int'
    - 'compression: float'
    - 'similarity: float'
    - 'structural: float'
    - 'success: bool'
    - 'error: Optional[str]'
  - n: ProjectResult
    d: Result for project reproduction.
    props:
    - 'project_path: str'
    - 'total_files: int'
    - 'successful_files: int'
    - 'failed_files: int'
    - 'total_source_chars: int'
    - 'total_logic_chars: int'
    - 'total_generated_chars: int'
    - 'avg_compression: float'
    - 'avg_similarity: float'
    - 'avg_structural: float'
    - 'files: List[FileResult]'
    - 'by_language: Dict[str,Dict[str,float]]'
  - n: ProjectReproducer
    d: Multi-file project reproduction system.
    m:
    - n: __init__
      sig: client:BaseLLMClient=None, max_workers:int=4, target_lang:str=None, use_llm:bool=True
      d: Initialize project reproducer.
    - n: _get_client
      sig: ''
      ret: BaseLLMClient
      d: or create LLM client.
    - n: find_source_files
      sig: project_path:str, extensions:Set[str]=None, exclude_patterns:List[str]=None
      ret: List[Path]
      d: Find all source files in project.
    - n: reproduce_file
      sig: file_path:Path, output_dir:Path
      ret: FileResult
      d: Reproduce a single file.
    - n: reproduce_project
      sig: project_path:str, output_dir:str=None, parallel:bool=False
      ret: ProjectResult
      d: Reproduce entire project.
    - n: _aggregate_results
      sig: project_path:str, results:List[FileResult]
      ret: ProjectResult
      d: Aggregate file results into project result.
    - n: _save_report
      sig: output_dir:Path, result:ProjectResult
      d: Save project reproduction report.
  f:
  - n: reproduce_project
    sig: project_path:str, output_dir:str=None, target_lang:str=None, parallel:bool=False, use_llm:bool=True
    ret: ProjectResult
    d: Convenience function for project reproduction.
  const:
  - n: SUPPORTED_EXTENSIONS
- p: base.py
  l: 50
  i:
  - logging
  - typing.Optional
  e:
  - VerboseMixin
  - BaseParser
  - BaseGenerator
  c:
  - n: VerboseMixin
    d: Mixin providing verbose logging functionality.
    m:
    - n: __init__
      sig: ''
      d: creates
    - n: log
      sig: msg:str
      d: logs
    - n: debug
      sig: msg:str
      d: debug
    - n: info
      sig: msg:str
      d: info
    - n: warn
      sig: msg:str
      d: warn
    - n: error
      sig: msg:str
      d: error
  - n: BaseParser
    b:
    - VerboseMixin
    d: Base class for code parsers.
    m:
    - n: __init__
      sig: ''
      d: creates
    - n: parse
      sig: content:str
      d: parses
    - n: parse_file
      sig: path:str
      d: parses file
  - n: BaseGenerator
    b:
    - VerboseMixin
    d: Base class for output generators.
    m:
    - n: __init__
      sig: ''
      d: creates
    - n: generate
      sig: project
      ret: str
      d: creates
- p: cli.py
  l: 767
  i:
  - argparse
  - os
  - sys
  - subprocess
  - time
  - logging
  - json
  - signal
  - datetime
  - __version__
  e:
  - Colors
  - Logger
  - ensure_dependencies
  - main
  c:
  - n: Colors
    props:
    - BLUE
    - GREEN
    - YELLOW
    - RED
    - CYAN
    - BOLD
    - DIM
    - NC
  - n: Logger
    d: Enhanced logger for CLI output.
    m:
    - n: __init__
      sig: verbose:bool=False, debug:bool=False
      d: creates
    - n: _elapsed
      sig: ''
      ret: str
      d: elapsed time string.
    - n: info
      sig: msg:str
      d: Print info message.
    - n: success
      sig: msg:str
      d: Print success message.
    - n: warning
      sig: msg:str
      d: Print warning message.
    - n: error
      sig: msg:str
      d: Print error message.
    - n: step
      sig: msg:str
      d: Print step message with counter.
    - n: detail
      sig: msg:str
      d: Print detail message (only in verbose mode).
    - n: debug_msg
      sig: msg:str
      d: Print debug message (only in debug mode).
    - n: stats
      sig: label:str, value
      d: Print statistics.
    - n: separator
      sig: ''
      d: Print separator line.
    - n: header
      sig: msg:str
      d: Print header.
  f:
  - n: ensure_dependencies
    sig: ''
    d: Auto-install optional dependencies for best result
  - n: _get_env_file_path
    sig: ''
    ret: str
    d: retrieves env file path
  - n: _read_text_file
    sig: path:str
    ret: str
    d: retrieves text file
  - n: _write_text_file
    sig: path:str, content:str
    ret: None
    d: logs text file
  - n: _set_env_var
    sig: var_name:str, value:str
    ret: str
    d: updates env var
  - n: _unset_env_var
    sig: var_name:str
    ret: str
    d: unset env var
  - n: _get_litellm_config_path
    sig: ''
    ret: str
    d: retrieves litellm config path
  - n: _get_user_llm_config_path
    sig: ''
    ret: str
    d: retrieves user llm config path
  - n: _load_user_llm_config
    sig: ''
    ret: dict
    d: retrieves user llm config
  - n: _save_user_llm_config
    sig: data:dict
    ret: str
    d: caches user llm config
  - n: _load_litellm_yaml
    sig: ''
    ret: dict
    d: retrieves litellm yaml
  - n: _save_litellm_yaml
    sig: data:dict
    ret: str
    d: caches litellm yaml
  - n: _infer_provider_from_litellm_model
    sig: litellm_model:str
    ret: str
    d: infer provider from litellm model
  - n: _code2logic_llm_cli
    sig: argv:list[str]
    ret: None
    d: code2logic llm cli
  - n: main
    sig: ''
    d: Main CLI entry point.
- p: llm.py
  l: 376
  i:
  - json
  - os
  - dataclasses.dataclass
  - llm_clients.{BaseLLMClient,LiteLLMClient,OllamaLocalClient,OpenRouterClient,get_client}
  - typing.{Any,Dict,List,Optional}
  e:
  - LLMConfig
  - OllamaClient
  - LiteLLMClient
  - CodeAnalyzer
  - get_available_backends
  c:
  - n: LLMConfig
    d: Configuration for LLM backend.
    props:
    - 'provider: str'
    - 'model: str'
    - 'base_url: str'
    - 'api_key: Optional[str]'
    - 'timeout: int'
    - 'temperature: float'
    - 'max_tokens: int'
  - n: OllamaClient
    d: Direct Ollama API client.
    m:
    - n: __init__
      sig: config:LLMConfig
      d: creates
    - n: generate
      sig: prompt:str, system:Optional[str]=None
      ret: str
      d: Generate completion from Ollama.
    - n: chat
      sig: messages:List[Dict[str,str]]
      ret: str
      d: Chat completion from Ollama.
    - n: is_available
      sig: ''
      ret: bool
      d: Check if Ollama is running.
    - n: list_models
      sig: ''
      ret: List[str]
      d: List available models.
  - n: LiteLLMClient
    d: LiteLLM client for unified API access.
    m:
    - n: __init__
      sig: config:LLMConfig
      d: creates
    - n: generate
      sig: prompt:str, system:Optional[str]=None
      ret: str
      d: Generate completion via LiteLLM.
    - n: chat
      sig: messages:List[Dict[str,str]]
      ret: str
      d: Chat completion via LiteLLM.
    - n: is_available
      sig: ''
      ret: bool
      d: Check if LiteLLM backend is available.
  - n: CodeAnalyzer
    d: LLM-powered code analysis for Code2Logic.
    props:
    - SYSTEM_PROMPT
    m:
    - n: __init__
      sig: model:str=None, provider:str=None, base_url:str=None, api_key:str=None
      d: Initialize CodeAnalyzer.
    - n: is_available
      sig: ''
      ret: bool
      d: Check if LLM backend is available.
    - n: suggest_refactoring
      sig: project
      ret: List[Dict[str,Any]]
      d: Analyze project and suggest refactoring improvemen
    - n: find_semantic_duplicates
      sig: project
      ret: List[Dict[str,Any]]
      d: Find semantically similar functions using LLM.
    - n: generate_code
      sig: project, target_lang:str, module_filter:Optional[str]=None
      ret: Dict[str,str]
      d: Generate code in target language from project anal
    - n: translate_function
      sig: name:str, signature:str, intent:str, source_lang:str, target_lang:str
      ret: str
      d: Translate a single function to another language.
    - n: _build_signature
      sig: f
      ret: str
      d: Build compact signature.
  f:
  - n: get_available_backends
    sig: ''
    ret: Dict[str,bool]
    d: availability status of LLM backends.
- p: errors.py
  l: 372
  i:
  - logging
  - traceback
  - dataclasses.{dataclass,field}
  - enum.Enum
  - pathlib.Path
  - typing.{Any,Callable,Dict,List,Optional}
  e:
  - ErrorSeverity
  - ErrorType
  - AnalysisError
  - AnalysisResult
  - ErrorHandler
  - create_error_handler
  c:
  - n: ErrorSeverity
    b:
    - Enum
    d: Error severity levels.
    props:
    - WARNING
    - ERROR
    - CRITICAL
  - n: ErrorType
    b:
    - Enum
    d: Types of errors that can occur during analysis.
    props:
    - FILE_NOT_FOUND
    - PERMISSION_DENIED
    - FILE_TOO_LARGE
    - ENCODING_ERROR
    - SYMLINK_LOOP
    - DISK_FULL
    - PATH_TOO_LONG
    - SYNTAX_ERROR
    - PARSE_TIMEOUT
    - UNSUPPORTED_LANGUAGE
    - BINARY_FILE
    - EMPTY_FILE
    - YAML_SERIALIZATION
    - JSON_SERIALIZATION
    - OUTPUT_WRITE_ERROR
  - n: AnalysisError
    d: Represents an error during analysis.
    props:
    - 'type: ErrorType'
    - 'severity: ErrorSeverity'
    - 'path: str'
    - 'message: str'
    - 'exception: Optional[str]'
    - 'suggestion: str'
    m:
    - n: to_dict
      sig: ''
      ret: Dict[str,Any]
      d: converts dict
  - n: AnalysisResult
    d: Result of analysis with errors tracked.
    props:
    - 'success: bool'
    - 'errors: List[AnalysisError]'
    - 'warnings: List[AnalysisError]'
    - 'skipped_files: List[str]'
    - 'processed_files: int'
    - 'total_files: int'
    m:
    - n: add_error
      sig: error:AnalysisError
      d: Add an error to the result.
    - n: has_errors
      sig: ''
      ret: bool
      d: has errors
    - n: summary
      sig: ''
      ret: str
      d: Generate error summary.
  - n: ErrorHandler
    d: Handles errors during analysis with configurable behavior.
    props:
    - SUGGESTIONS
    m:
    - n: __init__
      sig: mode:str='lenient', max_file_size_mb:float=10.0, timeout_seconds:float=30.0, logger:Optional[Any]=None
      d: creates
    - n: reset
      sig: ''
      d: Reset error state for new analysis.
    - n: handle_error
      sig: error_type:ErrorType, path:str, message:str, exception:Optional[Exception]=None, severity:Optional[ErrorSeverity]=None
      ret: bool
      d: Handle an error.
    - n: _default_severity
      sig: error_type:ErrorType
      ret: ErrorSeverity
      d: default severity for error type.
    - n: _log_error
      sig: error:AnalysisError
      d: Log an error.
    - n: safe_read_file
      sig: path:Path
      ret: Optional[str]
      d: Safely read a file with error handling.
    - n: safe_write_file
      sig: path:Path, content:str
      ret: bool
      d: Safely write a file with error handling.
    - n: safe_parse
      sig: path:str, content:str, parser_func:Callable
      ret: Any
      d: Safely parse content with error handling.
  f:
  - n: create_error_handler
    sig: mode:str='lenient', max_file_size_mb:float=10.0
    ret: ErrorHandler
    d: an error handler with default settings.
- p: code_review.py
  l: 205
  i:
  - collections.defaultdict
  - typing.{Any,Dict,List}
  e:
  - CodeReviewer
  - analyze_code_quality
  - check_security_issues
  - check_performance_issues
  c:
  - n: CodeReviewer
    d: Automated code review with optional LLM enhancement.
    m:
    - n: __init__
      sig: client=None
      d: Initialize reviewer.
    - n: review
      sig: project, focus:str='all'
      ret: Dict[str,Any]
      d: Perform code review.
    - n: generate_report
      sig: results:Dict[str,Any], project_name:str='Project'
      ret: str
      d: Generate markdown review report.
  f:
  - n: analyze_code_quality
    sig: project
    ret: Dict[str,List[Dict]]
    d: Analyze code quality issues.
  - n: check_security_issues
    sig: project
    ret: Dict[str,List[Dict]]
    d: Check for security vulnerabilities.
  - n: check_performance_issues
    sig: project
    ret: Dict[str,List[Dict]]
    d: Check for performance anti-patterns.
  const:
  - n: SECURITY_PATTERNS
  - n: PERFORMANCE_PATTERNS
  - n: COMPLEXITY_HIGH
  - n: COMPLEXITY_MEDIUM
  - n: LINES_MAX
  - n: FILE_LINES_MAX
- p: analyzer.py
  l: 230
  i:
  - sys
  - datetime
  - collections.defaultdict
  - dependency.{DependencyAnalyzer,NETWORKX_AVAILABLE}
  - models.{ModuleInfo,ProjectInfo}
  - parsers.{TREE_SITTER_AVAILABLE,TreeSitterParser,UniversalParser}
  - pathlib.Path
  - similarity.SimilarityDetector
  - typing.{Dict,List,Optional}
  e:
  - ProjectAnalyzer
  - analyze_project
  - get_library_status
  c:
  - n: ProjectAnalyzer
    d: Main class for analyzing software projects.
    props:
    - 'LANGUAGE_EXTENSIONS: Dict[str,str]'
    - 'IGNORE_DIRS: set'
    - 'IGNORE_FILES: set'
    m:
    - n: __init__
      sig: root_path:str, use_treesitter:bool=True, verbose:bool=False, include_private:bool=False
      d: Initialize the project analyzer.
    - n: _print_status
      sig: ''
      d: Print library availability status.
    - n: analyze
      sig: ''
      ret: ProjectInfo
      d: Analyze the project.
    - n: _scan_files
      sig: ''
      d: Scan and parse all source files.
    - n: _detect_entrypoints
      sig: ''
      ret: List[str]
      d: Detect project entry points.
    - n: get_statistics
      sig: ''
      ret: Dict
      d: analysis statistics.
  f:
  - n: analyze_project
    sig: path:str, use_treesitter:bool=True, verbose:bool=False
    ret: ProjectInfo
    d: Convenience function to analyze a project.
  - n: get_library_status
    sig: ''
    ret: Dict[str,bool]
    d: availability status of optional libraries.
- p: quality.py
  l: 212
  i:
  - dataclasses.{dataclass,field}
  - models.{ModuleInfo,ProjectInfo}
  - typing.{Any,Dict,List}
  e:
  - QualityIssue
  - QualityReport
  - QualityAnalyzer
  - analyze_quality
  - get_quality_summary
  c:
  - n: QualityIssue
    d: Represents a code quality issue.
    props:
    - 'type: str'
    - 'severity: str'
    - 'file: str'
    - 'name: str'
    - 'value: int'
    - 'threshold: int'
    - 'recommendation: str'
  - n: QualityReport
    d: Complete quality analysis report.
    props:
    - 'issues: List[QualityIssue]'
    - 'metrics: Dict[str,Any]'
    - 'score: float'
    m:
    - n: to_dict
      sig: ''
      ret: Dict[str,Any]
      d: Convert to dictionary.
  - n: QualityAnalyzer
    d: Analyzes code quality and generates recommendations.
    props:
    - DEFAULT_THRESHOLDS
    m:
    - n: __init__
      sig: thresholds:Dict[str,int]=None
      d: Initialize with custom thresholds.
    - n: analyze
      sig: project:ProjectInfo
      ret: QualityReport
      d: Analyze project quality.
    - n: analyze_modules
      sig: modules:List[ModuleInfo]
      ret: QualityReport
      d: Analyze a list of modules.
    - n: _analyze_module
      sig: module:ModuleInfo, report:QualityReport
      d: Analyze a single module.
    - n: _check_function
      sig: func, file_path:str, report:QualityReport
      d: Check function quality.
    - n: _check_class
      sig: file_path:str, report:QualityReport
      d: Check class quality.
    - n: _get_file_recommendation
      sig: module:ModuleInfo
      ret: str
      d: Generate recommendation for long file.
  f:
  - n: analyze_quality
    sig: project:ProjectInfo, thresholds:Dict[str,int]=None
    ret: QualityReport
    d: Convenience function to analyze project quality.
  - n: get_quality_summary
    sig: report:QualityReport
    ret: str
    d: Generate human-readable quality summary.
- p: shared_utils.py
  l: 279
  i:
  - hashlib
  - re
  - typing.{Dict,List,Optional,Set}
  e:
  - compact_imports
  - deduplicate_imports
  - abbreviate_type
  - expand_type
  - build_signature
  - remove_self_from_params
  - categorize_function
  - extract_domain
  - compute_hash
  - truncate_docstring
  f:
  - n: compact_imports
    sig: imports:List[str], max_items:int=10
    ret: List[str]
    d: Compact imports by grouping submodules.
  - n: deduplicate_imports
    sig: imports:List[str]
    ret: List[str]
    d: Remove redundant imports.
  - n: abbreviate_type
    sig: type_str:str
    ret: str
    d: Abbreviate type annotations for compactness.
  - n: expand_type
    sig: abbrev:str
    ret: str
    d: Expand abbreviated type back to full form.
  - n: build_signature
    sig: params:List[str], return_type:Optional[str]=None, include_self:bool=False, abbreviate:bool=False, max_params:int=6
    ret: str
    d: Build compact function signature.
  - n: remove_self_from_params
    sig: params:List[str]
    ret: List[str]
    d: Remove 'self' and 'cls' from parameter list.
  - n: categorize_function
    sig: name:str
    ret: str
    d: Categorize function by name pattern.
  - n: extract_domain
    sig: path:str
    ret: str
    d: Extract domain from file path.
  - n: compute_hash
    sig: name:str, signature:str, length:int=8
    ret: str
    d: Compute short hash for quick comparison.
  - n: truncate_docstring
    sig: docstring:Optional[str], max_length:int=60
    ret: str
    d: Truncate docstring to first sentence or max_length
  - n: escape_for_yaml
    sig: text:str
    ret: str
    d: Escape text for safe YAML inclusion.
  - n: clean_identifier
    sig: name:str
    ret: str
    d: Clean identifier by removing whitespace and specia
  const:
  - n: TYPE_ABBREVIATIONS
  - n: CATEGORY_PATTERNS
  - n: DOMAIN_KEYWORDS
- p: parsers.py
  l: 1114
  i:
  - ast
  - re
  - intent.EnhancedIntentGenerator
  - models
  - typing.{List,Optional}
  e:
  - TreeSitterParser
  - UniversalParser
  - is_tree_sitter_available
  c:
  - n: TreeSitterParser
    d: Parser using Tree-sitter for high-accuracy AST parsing.
    m:
    - n: __init__
      sig: ''
      d: Initialize Tree-sitter parsers for available langu
    - n: _init_parsers
      sig: ''
      d: Initialize parsers for each supported language.
    - n: is_available
      sig: language:str
      ret: bool
      d: Check if Tree-sitter parser is available for a lan
    - n: get_supported_languages
      sig: ''
      ret: List[str]
      d: list of potentially supported languages.
    - n: parse
      sig: filepath:str, content:str, language:str
      ret: Optional[ModuleInfo]
      d: Parse a source file using Tree-sitter.
    - n: _parse_python
      sig: filepath:str, content:str, tree
      ret: ModuleInfo
      d: Parse Python source using Tree-sitter AST.
    - n: _extract_constants
      sig: tree, content:str
      ret: List[ConstantInfo]
      d: Extract module-level UPPERCASE constants.
    - n: _extract_type_checking_imports
      sig: tree, content:str
      ret: List[str]
      d: Extract TYPE_CHECKING block imports.
    - n: _extract_conditional_imports
      sig: node, content:str
      ret: List[str]
      d: Extract imports from try/except blocks.
    - n: _extract_aliases
      sig: tree, content:str
      ret: dict
      d: Extract module aliases (import X as Y).
    - n: _extract_py_function
      sig: node, content:str, decorated_node=None
      ret: Optional[FunctionInfo]
      d: Extract Python function from AST node.
    - n: _extract_py_class
      sig: node, content:str
      ret: Optional[ClassInfo]
      d: Extract Python class from AST node.
  - n: UniversalParser
    d: Fallback parser using Python AST and regex.
    m:
    - n: __init__
      sig: ''
      d: Initialize the universal parser.
    - n: parse
      sig: filepath:str, content:str, language:str
      ret: Optional[ModuleInfo]
      d: Parse a source file using AST or regex.
    - n: _parse_python
      sig: filepath:str, content:str
      ret: Optional[ModuleInfo]
      d: Parse Python using built-in AST.
    - n: _extract_ast_function
      sig: node
      ret: FunctionInfo
      d: Extract function from Python AST node.
    - n: _extract_ast_class
      sig: node:Any
      ret: ClassInfo
      d: Extract class from Python AST node.
    - n: _ann_str
      sig: node
      ret: str
      d: Convert AST annotation to string.
    - n: _parse_js_ts
      sig: filepath:str, content:str, language:str
      ret: ModuleInfo
      d: Parse JS/TS using regex patterns.
  f:
  - n: _normalize_import_path
    sig: import_path:str
    ret: str
    d: Normalize import path by removing duplicate suffix
  - n: _clean_imports
    sig: imports:List[str]
    ret: List[str]
    d: Deduplicate and normalize import paths while prese
  - n: _combine_import_name
    sig: module_name:str, identifier:str
    ret: str
    d: Combine module and identifier while avoiding dupli
  - n: is_tree_sitter_available
    sig: ''
    ret: bool
    d: Check if Tree-sitter is available.
  const:
  - n: TREE_SITTER_AVAILABLE
- p: intent.py
  l: 429
  i:
  - re
  - spacy
  - dataclasses.{dataclass,field}
  - enum.{Enum,auto}
  - nltk.stem.WordNetLemmatizer
  - typing.{Any,List,Optional,TYPE_CHECKING,Tuple}
  e:
  - IntentType
  - EnhancedIntentGenerator
  - IntentAnalyzer
  c:
  - n: IntentType
    b:
    - Enum
    d: Types of user intents for code analysis.
  - n: EnhancedIntentGenerator
    d: Generator intencji z NLP - lemmatyzacja, ekstrakcja z docstr
    m:
    - n: __init__
      sig: ''
      d: creates
    - n: generate
      sig: name:str
      ret: str
      d: creates
    - n: _extract_from_docstring
      sig: docstring:str
      ret: Optional[str]
      d: parses from docstring
    - n: _split_name
      sig: name:str
      ret: List[str]
      d: splits name
    - n: get_available_features
      sig: ''
      ret: dict[str, bool]
      d: retrieves available features
  - n: IntentAnalyzer
    d: Analyzes user queries to detect intent and provide suggestio
    m:
    - n: __init__
      sig: ''
      d: creates
    - n: _extract_keywords
      sig: query:str
      ret: List[str]
      d: parses keywords
    - n: _calculate_intent_confidence
      sig: keywords:List[str], patterns:List[str]
      ret: float
      d: processes intent confidence
    - n: _identify_target
      sig: query:str, project:Any
      ret: str
      d: identify target
    - n: _generate_description
      sig: intent_type:IntentType, target:str
      ret: str
      d: creates description
    - n: _generate_suggestions
      sig: intent_type:IntentType, target:str, project:Any
      ret: List[str]
      d: creates suggestions
    - n: analyze_intent
      sig: query:str, project:Any
      ret: List[Intent]
      d: processes intent
    - n: detect_code_smells
      sig: project:Any
      ret: List[dict]
      d: detect code smells
    - n: suggest_refactoring
      sig: target:str, project:Any
      ret: List[str]
      d: suggest refactoring
    - n: _find_target_object
      sig: target:str, project:Any
      ret: Any
      d: retrieves target object
    - n: _suggest_module_refactoring
      sig: module:Any
      ret: List[str]
      d: suggest module refactoring
    - n: _suggest_class_refactoring
      sig: cls:Any
      ret: List[str]
      d: suggest class refactoring
- p: adaptive.py
  l: 475
  i:
  - os
  - re
  - dataclasses.dataclass
  - file_formats.{generate_file_csv,generate_file_json}
  - llm_clients.{BaseLLMClient,get_client}
  - pathlib.Path
  - reproduction.{compare_code,extract_code_block}
  - typing.{Any,Dict,List,Optional,Tuple}
  e:
  - ChunkInfo
  - AdaptiveResult
  - AdaptiveReproducer
  - get_llm_capabilities
  c:
  - n: ChunkInfo
    d: Information about a code chunk.
    props:
    - 'index: int'
    - 'total: int'
    - 'content: str'
    - 'element_type: str'
    - 'element_name: str'
  - n: AdaptiveResult
    d: Result of adaptive reproduction.
    props:
    - 'source_file: str'
    - 'source_chars: int'
    - 'format_used: str'
    - 'chunks_used: int'
    - 'spec_chars: int'
    - 'generated_chars: int'
    - 'similarity: float'
    - 'structural_score: float'
    - 'compression_ratio: float'
    - 'efficiency_score: float'
  - n: AdaptiveReproducer
    d: Adaptive code reproduction with LLM capability detection.
    m:
    - n: __init__
      sig: client:BaseLLMClient=None, model:str=None
      d: Initialize adaptive reproducer.
    - n: _get_capabilities
      sig: ''
      ret: Dict[str,Any]
      d: LLM capabilities for current model.
    - n: select_format
      sig: file_path:Path, content:str
      ret: str
      d: Select optimal format based on file and LLM capabi
    - n: should_chunk
      sig: content:str
      ret: bool
      d: Determine if content should be chunked.
    - n: chunk_content
      sig: content:str, file_path:Path
      ret: List[ChunkInfo]
      d: Split content into logical chunks.
    - n: generate_chunk_spec
      sig: chunk:ChunkInfo, format_name:str
      ret: str
      d: Generate specification for a single chunk.
    - n: _gherkin_for_chunk
      sig: chunk:ChunkInfo
      ret: str
      d: Generate Gherkin for a chunk.
    - n: _yaml_for_chunk
      sig: chunk:ChunkInfo
      ret: str
      d: Generate YAML for a chunk.
    - n: _json_for_chunk
      sig: chunk:ChunkInfo
      ret: str
      d: Generate JSON for a chunk.
    - n: reproduce
      sig: file_path:str, output_dir:str=None
      ret: AdaptiveResult
      d: Reproduce code with adaptive format selection.
    - n: _reproduce_single
      sig: path:Path, content:str, format_name:str, output_dir:str=None
      ret: AdaptiveResult
      d: Reproduce without chunking.
    - n: _reproduce_chunked
      sig: path:Path, content:str, format_name:str, output_dir:str=None
      ret: AdaptiveResult
      d: Reproduce with chunking.
  f:
  - n: get_llm_capabilities
    sig: model:str
    ret: Dict[str,Any]
    d: capabilities for a specific model.
  const:
  - n: LLM_CAPABILITIES
- p: reproducer.py
  l: 534
  i:
  - os
  - yaml
  - json
  - re
  - dataclasses.{dataclass,field}
  - enum.Enum
  - pathlib.Path
  - typing.{Any,Callable,Dict,List,Optional}
  e:
  - ReproductionStatus
  - FileValidation
  - ReproductionResult
  - SpecReproducer
  - SpecValidator
  - reproduce_project
  - validate_files
  c:
  - n: ReproductionStatus
    b:
    - Enum
    d: Status of file reproduction.
    props:
    - SUCCESS
    - PARTIAL
    - FAILED
    - SKIPPED
  - n: FileValidation
    d: Validation result for a single file.
    props:
    - 'path: str'
    - 'exists: bool'
    - 'syntax_ok: bool'
    - 'structure_match: bool'
    - 'classes_match: int'
    - 'classes_expected: int'
    - 'functions_match: int'
    - 'functions_expected: int'
    - 'errors: List[str]'
    m:
    - n: score
      sig: ''
      ret: float
      d: Calculate match score 0-100.
    - n: to_dict
      sig: ''
      ret: Dict[str,Any]
      d: converts dict
  - n: ReproductionResult
    d: Result of reproduction process.
    props:
    - 'output_dir: str'
    - 'total_files: int'
    - 'generated_files: int'
    - 'failed_files: int'
    - 'skipped_files: int'
    - 'validations: List[FileValidation]'
    - 'errors: List[str]'
    m:
    - n: success_rate
      sig: ''
      ret: float
      d: success rate
    - n: average_score
      sig: ''
      ret: float
      d: average score
    - n: summary
      sig: ''
      ret: str
      d: summary
  - n: SpecReproducer
    d: Reproduces code structure from logic specifications.
    m:
    - n: __init__
      sig: verbose:bool=False
      d: creates
    - n: reproduce_from_yaml
      sig: spec_path:str, output_dir:str, filter_paths:Optional[List[str]]=None
      ret: ReproductionResult
      d: Reproduce files from YAML specification.
    - n: reproduce_from_json
      sig: spec_path:str, output_dir:str, filter_paths:Optional[List[str]]=None
      ret: ReproductionResult
      d: Reproduce files from JSON specification.
    - n: _reproduce
      sig: spec:Dict[str,Any], output_dir:str, filter_paths:Optional[List[str]]=None
      ret: ReproductionResult
      d: Internal reproduction logic.
    - n: _generate_file
      sig: module:Dict[str,Any], output_path:Path
      ret: bool
      d: Generate a single file from module spec.
    - n: _generate_python
      sig: module:Dict[str,Any]
      ret: str
      d: Generate Python file content.
    - n: _render_docstring
      sig: text:str, indent:str
      ret: List[str]
      d: Render a safe Python docstring or fall back to com
    - n: _sanitize_python_property
      sig: prop:str
      ret: str
      d: Sanitize a Python class property declaration for v
    - n: _generate_python_class
      sig: cls:Dict[str,Any]
      ret: List[str]
      d: Generate Python class.
    - n: _generate_python_method
      sig: method:Dict[str,Any]
      ret: List[str]
      d: Generate Python method.
    - n: _generate_python_function
      sig: func:Dict[str,Any]
      ret: List[str]
      d: Generate Python function.
    - n: _generate_typescript
      sig: module:Dict[str,Any]
      ret: str
      d: Generate TypeScript file content.
  - n: SpecValidator
    d: Validates generated files against logic specification.
    m:
    - n: __init__
      sig: ''
      d: creates
    - n: validate
      sig: spec_path:str, generated_dir:str, filter_paths:Optional[List[str]]=None
      ret: List[FileValidation]
      d: Validate generated files against spec.
    - n: _validate_file
      sig: module:Dict[str,Any], base_path:Path
      ret: FileValidation
      d: Validate a single file.
    - n: _check_python_syntax
      sig: content:str, validation:FileValidation
      ret: bool
      d: Check Python syntax.
  f:
  - n: reproduce_project
    sig: spec_path:str, output_dir:str, filter_paths:Optional[List[str]]=None, validate:bool=True, verbose:bool=True
    ret: ReproductionResult
    d: Convenience function to reproduce and validate a p
  - n: validate_files
    sig: spec_path:str, generated_dir:str, filter_paths:Optional[List[str]]=None
    ret: List[FileValidation]
    d: Validate specific files against spec.
- p: llm_clients.py
  l: 415
  i:
  - os
  - json
  - abc.{ABC,abstractmethod}
  - typing.{Any,Dict,List,Optional}
  e:
  - BaseLLMClient
  - OpenRouterClient
  - OllamaLocalClient
  - LiteLLMClient
  - get_priority_mode
  - get_client
  - get_effective_provider_priorities
  c:
  - n: BaseLLMClient
    b:
    - ABC
    d: Abstract base class for LLM clients.
    m:
    - n: generate
      sig: prompt:str, system:str=None, max_tokens:int=4000
      ret: str
      d: Generate completion.
    - n: is_available
      sig: ''
      ret: bool
      d: Check if client is available.
    - n: chat
      sig: messages:List[Dict[str,str]], max_tokens:int=4000
      ret: str
      d: Chat completion (default implementation).
  - n: OpenRouterClient
    b:
    - BaseLLMClient
    d: OpenRouter API client for cloud LLM access.
    props:
    - API_URL
    - provider
    m:
    - n: __init__
      sig: api_key:str=None, model:str=None
      d: Initialize OpenRouter client.
    - n: generate
      sig: prompt:str, system:str=None, max_tokens:int=4000
      ret: str
      d: Generate completion using OpenRouter.
    - n: is_available
      sig: ''
      ret: bool
      d: Check if OpenRouter is configured.
    - n: list_recommended_models
      sig: ''
      ret: List[tuple]
      d: List recommended models for code tasks.
  - n: OllamaLocalClient
    b:
    - BaseLLMClient
    d: Ollama client for local LLM inference.
    props:
    - provider
    m:
    - n: __init__
      sig: model:str=None, host:str=None
      d: Initialize Ollama client.
    - n: generate
      sig: prompt:str, system:str=None, max_tokens:int=4000
      ret: str
      d: Generate completion using Ollama.
    - n: is_available
      sig: ''
      ret: bool
      d: Check if Ollama is running.
    - n: list_models
      sig: ''
      ret: List[str]
      d: List available Ollama models.
    - n: list_recommended_models
      sig: ''
      ret: List[tuple]
      d: List recommended models for code tasks.
  - n: LiteLLMClient
    b:
    - BaseLLMClient
    d: LiteLLM client for universal LLM access.
    props:
    - provider
    m:
    - n: __init__
      sig: model:str=None
      d: Initialize LiteLLM client.
    - n: generate
      sig: prompt:str, system:str=None, max_tokens:int=4000
      ret: str
      d: Generate completion using LiteLLM.
    - n: is_available
      sig: ''
      ret: bool
      d: Check if LiteLLM is available.
  f:
  - n: _get_user_llm_config_path
    sig: ''
    ret: str
    d: retrieves user llm config path
  - n: _load_user_llm_config
    sig: ''
    ret: Dict[str,Any]
    d: retrieves user llm config
  - n: _get_priority_mode
    sig: ''
    ret: str
    d: retrieves priority mode
  - n: get_priority_mode
    sig: ''
    ret: str
    d: retrieves priority mode
  - n: _get_provider_priority_overrides
    sig: ''
    ret: Dict[str,int]
    d: retrieves provider priority overrides
  - n: _get_model_priority_rules
    sig: ''
    ret: Dict[str,Dict[str,int]]
    d: retrieves model priority rules
  - n: _get_model_priority
    sig: model_string:str
    ret: Optional[int]
    d: retrieves model priority
  - n: _get_provider_model_string
    sig: provider:str
    ret: str
    d: retrieves provider model string
  - n: get_client
    sig: provider:str=None, model:str=None
    ret: BaseLLMClient
    d: appropriate LLM client based on provider.
  - n: _try_client
    sig: provider:str, model:str=None
    ret: Optional[BaseLLMClient]
    d: try client
  - n: _get_priority_order
    sig: ''
    ret: List[str]
    d: retrieves priority order
  - n: _get_effective_provider_order
    sig: ''
    ret: List[tuple[str,int]]
    d: retrieves effective provider order
  - n: get_effective_provider_priorities
    sig: ''
    ret: Dict[str,int]
    d: retrieves effective provider priorities
  - n: _get_provider_priorities_from_litellm_yaml
    sig: ''
    ret: Dict[str,int]
    d: retrieves provider priorities from litellm yaml
  - n: _candidate_litellm_yaml_paths
    sig: ''
    ret: List[str]
    d: candidate litellm yaml paths
  const:
  - n: RECOMMENDED_MODELS
  - n: DEFAULT_MODELS
  - n: DEFAULT_PROVIDER_PRIORITIES
- p: prompts.py
  l: 120
  i:
  - typing.Dict
  e:
  - get_reproduction_prompt
  - get_review_prompt
  - get_fix_prompt
  f:
  - n: get_reproduction_prompt
    sig: spec:str, fmt:str, file_name:str, language:str='python', max_spec_length:int=5000
    ret: str
    d: Generate optimized reproduction prompt.
  - n: get_review_prompt
    sig: code:str, spec:str, fmt:str
    ret: str
    d: Generate code review prompt.
  - n: get_fix_prompt
    sig: code:str, issues:list, spec:str
    ret: str
    d: Generate code fix prompt.
- p: chunked_reproduction.py
  l: 357
  i:
  - re
  - dataclasses.{dataclass,field}
  - models.{ClassInfo,FunctionInfo,ModuleInfo,ProjectInfo}
  - pathlib.Path
  - typing.{Dict,List,Optional,Tuple}
  - utils.estimate_tokens
  e:
  - Chunk
  - ChunkedSpec
  - ChunkedResult
  - ChunkedReproducer
  - get_llm_limit
  - chunk_yaml_spec
  - chunk_gherkin_spec
  - chunk_markdown_spec
  - chunk_spec
  - get_chunk_prompt
  c:
  - n: Chunk
    d: A chunk of specification for reproduction.
    props:
    - 'id: int'
    - 'content: str'
    - 'tokens: int'
    - 'elements: List[str]'
    - 'dependencies: List[str]'
  - n: ChunkedSpec
    d: Chunked specification.
    props:
    - 'chunks: List[Chunk]'
    - 'total_tokens: int'
    - 'format: str'
    - 'file_name: str'
  - n: ChunkedResult
    d: Result of chunked reproduction.
    props:
    - 'file_name: str'
    - 'chunks_total: int'
    - 'chunks_success: int'
    - 'merged_code: str'
    - 'chunk_codes: List[str]'
    - 'errors: List[str]'
  - n: ChunkedReproducer
    d: Reproduce code from chunked specifications.
    m:
    - n: __init__
      sig: client, model_name:str='default'
      d: creates
    - n: reproduce
      sig: spec:str, fmt:str, file_name:str
      ret: ChunkedResult
      d: Reproduce code from specification, chunking if nee
    - n: _extract_code
      sig: response:str
      ret: str
      d: Extract code from LLM response.
  f:
  - n: get_llm_limit
    sig: model_name:str
    ret: int
    d: context limit for LLM model.
  - n: chunk_yaml_spec
    sig: spec:str, max_tokens:int=2000
    ret: List[Chunk]
    d: Chunk YAML specification by modules/classes/functi
  - n: chunk_gherkin_spec
    sig: spec:str, max_tokens:int=2000
    ret: List[Chunk]
    d: Chunk Gherkin specification by Features/Scenarios.
  - n: chunk_markdown_spec
    sig: spec:str, max_tokens:int=2000
    ret: List[Chunk]
    d: Chunk Markdown specification by sections.
  - n: chunk_spec
    sig: spec:str, fmt:str, max_tokens:int=2000
    ret: ChunkedSpec
    d: Chunk specification based on format.
  - n: get_chunk_prompt
    sig: chunk:Chunk, fmt:str, file_name:str, chunk_num:int, total_chunks:int
    ret: str
    d: Generate prompt for a single chunk.
  - n: merge_chunk_codes
    sig: codes:List[str], file_name:str
    ret: str
    d: Merge code from multiple chunks.
  - n: auto_chunk_reproduce
    sig: spec:str, fmt:str, file_name:str, client, model_name:str='default'
    ret: ChunkedResult
    d: Auto-chunking reproduction with LLM adaptation.
  - n: adaptive_chunk_reproduce
    sig: spec:str, fmt:str, file_name:str, client, provider:str='unknown', model:str='unknown'
    ret: ChunkedResult
    d: Adaptive chunking reproduction using LLM profile.
  const:
  - n: LLM_CONTEXT_LIMITS
- p: __init__.py
  l: 320
  i:
  - analyzer.{ProjectAnalyzer,analyze_project}
  - generators.{CSVGenerator,CompactGenerator,JSONGenerator,MarkdownGenerator,YAMLGenerator}
  - gherkin.{GherkinGenerator,StepDefinitionGenerator}
  - models
  e:
  - analyze_quality
  - reproduce_project
  f:
  - n: analyze_quality
    sig: target
    d: processes quality
  - n: reproduce_project
    sig: source:str
    d: reproduce project
- p: metrics.py
  l: 447
  i:
  - re
  - difflib
  - hashlib
  - logging
  - collections.Counter
  - dataclasses.{asdict,dataclass,field}
  - typing.{Any,Dict,List,Optional,Tuple}
  e:
  - TextMetrics
  - StructuralMetrics
  - SemanticMetrics
  - FormatMetrics
  - ReproductionResult
  - ReproductionMetrics
  - analyze_reproduction
  - compare_formats
  c:
  - n: TextMetrics
    d: Text-level similarity metrics.
    props:
    - 'char_similarity: float'
    - 'line_similarity: float'
    - 'word_similarity: float'
    - 'levenshtein_ratio: float'
    - 'jaccard_similarity: float'
    - 'cosine_similarity: float'
    - 'diff_added: int'
    - 'diff_removed: int'
    - 'diff_changed: int'
  - n: StructuralMetrics
    d: Structural code metrics.
    props:
    - 'classes_original: int'
    - 'classes_generated: int'
    - 'classes_match: bool'
    - 'functions_original: int'
    - 'functions_generated: int'
    - 'functions_match: bool'
    - 'methods_original: int'
    - 'methods_generated: int'
    - 'methods_match: bool'
    - 'imports_original: int'
    - 'imports_generated: int'
    - 'imports_match: bool'
    - 'attributes_original: int'
    - 'attributes_generated: int'
    - 'attributes_match: bool'
  - n: SemanticMetrics
    d: Semantic preservation metrics.
    props:
    - 'naming_similarity: float'
    - 'docstring_present: float'
    - 'type_hints_present: float'
    - 'decorator_match: float'
    - 'signature_match: float'
    - 'intent_score: float'
  - n: FormatMetrics
    d: Format-specific efficiency metrics.
    props:
    - 'format_name: str'
    - 'spec_chars: int'
    - 'spec_lines: int'
    - 'spec_tokens: int'
    - 'original_chars: int'
    - 'generated_chars: int'
    - 'compression_ratio: float'
    - 'expansion_ratio: float'
    - 'efficiency_score: float'
    - 'token_cost_estimate: float'
  - n: ReproductionResult
    d: Complete reproduction analysis result.
    props:
    - 'source_file: str'
    - 'format_used: str'
    - 'timestamp: str'
    - 'text: TextMetrics'
    - 'structural: StructuralMetrics'
    - 'semantic: SemanticMetrics'
    - 'format: FormatMetrics'
    - 'overall_score: float'
    - 'quality_grade: str'
    - 'recommendations: List[str]'
    m:
    - n: to_dict
      sig: ''
      ret: Dict[str,Any]
      d: converts dict
    - n: to_report
      sig: ''
      ret: str
      d: Generate detailed markdown report.
  - n: ReproductionMetrics
    d: Analyze reproduction quality with multiple metrics.
    m:
    - n: __init__
      sig: verbose:bool=False
      d: creates
    - n: analyze
      sig: original:str, generated:str, spec:str='', format_name:str='', source_file:str=''
      ret: ReproductionResult
      d: Analyze reproduction quality.
    - n: _compute_text_metrics
      sig: original:str, generated:str
      ret: TextMetrics
      d: Compute text-level metrics.
    - n: _cosine_similarity
      sig: words1:List[str], words2:List[str]
      ret: float
      d: Compute cosine similarity between word lists.
    - n: _compute_structural_metrics
      sig: original:str, generated:str
      ret: StructuralMetrics
      d: Compute structural metrics.
    - n: _compute_semantic_metrics
      sig: original:str, generated:str
      ret: SemanticMetrics
      d: Compute semantic preservation metrics.
    - n: _compute_format_metrics
      sig: original:str, generated:str, spec:str, format_name:str
      ret: FormatMetrics
      d: Compute format efficiency metrics.
    - n: _compute_overall_score
      sig: result:ReproductionResult
      ret: float
      d: Compute weighted overall score.
    - n: _get_grade
      sig: score:float
      ret: str
      d: letter grade from score.
    - n: _generate_recommendations
      sig: result:ReproductionResult
      ret: List[str]
      d: Generate improvement recommendations.
  f:
  - n: analyze_reproduction
    sig: original:str, generated:str, spec:str='', format_name:str='', verbose:bool=False
    ret: ReproductionResult
    d: Convenience function for reproduction analysis.
  - n: compare_formats
    sig: original:str, results:Dict[str,Tuple[str,str]], verbose:bool=False
    ret: Dict[str,Any]
    d: Compare reproduction quality across formats.
- p: __main__.py
  l: 9
  i:
  - cli.main
- p: refactor.py
  l: 313
  i:
  - json
  - analyzer.analyze_project
  - code_review.{analyze_code_quality,check_performance_issues,check_security_issues}
  - dataclasses.{asdict,dataclass,field}
  - llm_clients.get_client
  - pathlib.Path
  - similarity.SimilarityDetector
  - typing.{Any,Dict,List,Optional}
  e:
  - DuplicateGroup
  - RefactoringSuggestion
  - RefactoringReport
  - find_duplicates
  - analyze_quality
  - suggest_refactoring
  - compare_codebases
  - quick_analyze
  c:
  - n: DuplicateGroup
    d: Group of duplicate functions.
    props:
    - 'hash: str'
    - 'functions: List[str]'
    - 'suggestion: str'
    - 'effort: str'
  - n: RefactoringSuggestion
    d: Single refactoring suggestion.
    props:
    - 'type: str'
    - 'severity: str'
    - 'location: str'
    - 'description: str'
    - 'suggestion: str'
    - 'effort: str'
  - n: RefactoringReport
    d: Complete refactoring analysis report.
    props:
    - 'project_path: str'
    - 'total_files: int'
    - 'total_functions: int'
    - 'duplicates: List[DuplicateGroup]'
    - 'quality_issues: List[RefactoringSuggestion]'
    - 'security_issues: List[RefactoringSuggestion]'
    - 'suggestions: List[RefactoringSuggestion]'
    m:
    - n: to_dict
      sig: ''
      ret: Dict[str,Any]
      d: converts dict
    - n: to_markdown
      sig: ''
      ret: str
      d: Generate markdown report.
  f:
  - n: find_duplicates
    sig: project_path:str, threshold:float=0.8
    ret: List[DuplicateGroup]
    d: Find duplicate functions in a project.
  - n: analyze_quality
    sig: project_path:str, include_security:bool=True, include_performance:bool=True
    ret: RefactoringReport
    d: Analyze code quality and generate refactoring repo
  - n: suggest_refactoring
    sig: project_path:str, use_llm:bool=False, client:BaseLLMClient=None
    ret: RefactoringReport
    d: Generate refactoring suggestions for a project.
  - n: compare_codebases
    sig: project1:str, project2:str
    ret: Dict[str,Any]
    d: Compare two codebases for similarities and differe
  - n: quick_analyze
    sig: project_path:str
    ret: Dict[str,Any]
    d: Quick analysis for a project.
- p: logicml.py
  l: 281
  i:
  - dataclasses.dataclass
  - models.{ClassInfo,FunctionInfo,ModuleInfo,ProjectInfo}
  - pathlib.Path
  - shared_utils.{compact_imports,remove_self_from_params,truncate_docstring}
  - typing.{Any,Dict,List,Optional,Set}
  e:
  - LogicMLSpec
  - LogicMLGenerator
  - generate_logicml
  c:
  - n: LogicMLSpec
    d: LogicML specification output.
    props:
    - 'content: str'
    - 'token_estimate: int'
    - 'file_count: int'
    - 'class_count: int'
    - 'function_count: int'
  - n: LogicMLGenerator
    d: Generates LogicML format - optimized for LLM code reproducti
    props:
    - 'FORMAT_NAME: str'
    - 'FILE_EXTENSION: str'
    - 'TOKEN_EFFICIENCY: float'
    - 'REPRODUCTION_FIDELITY: float'
    - 'STDLIB_MODULES: Set[str]'
    m:
    - n: __init__
      sig: verbose:bool=False
      ret: None
      d: creates
    - n: generate
      sig: project:ProjectInfo, detail:str='standard'
      ret: LogicMLSpec
      d: Generate LogicML specification for a project.
    - n: _generate_module
      sig: module:ModuleInfo, detail:str
      ret: str
      d: Generate LogicML for a single module.
    - n: _generate_imports
      sig: imports:List[str]
      ret: str
      d: Generate compact imports section.
    - n: _generate_class
      sig: cls:ClassInfo, detail:str
      ret: str
      d: Generate LogicML for a class.
    - n: _generate_method
      sig: method:FunctionInfo, detail:str, indent:int=2
      ret: str
      d: Generate LogicML for a method.
    - n: _generate_functions
      sig: functions:List[FunctionInfo], detail:str
      ret: str
      d: Generate LogicML for top-level functions.
    - n: _detect_side_effects
      sig: method:FunctionInfo
      ret: Optional[str]
      d: Detect side effects from method calls and name pat
  f:
  - n: generate_logicml
    sig: project:ProjectInfo, detail:str='standard'
    ret: str
    d: Convenience function to generate LogicML format.
  const:
  - n: LOGICML_EXAMPLE
- p: utils.py
  l: 16
  i:
  - shutil
  - __future__.annotations
  - pathlib.Path
  e:
  - estimate_tokens
  - write_text_atomic
  - cleanup_generated_root
  f:
  - n: estimate_tokens
    sig: text:str
    ret: int
    d: estimate tokens
  - n: write_text_atomic
    sig: path:Path, content:str
    ret: None
    d: logs text atomic
  - n: cleanup_generated_root
    sig: generated_root:Path, allowed_dirs:set[str]
    ret: None
    d: cleanup generated root
- p: generators.py
  l: 1568
  i:
  - json
  - collections.defaultdict
  - models
  - pathlib.Path
  - shared_utils.{categorize_function,compact_imports,compute_hash,extract_domain,remove_self_from_params}
  - typing.List
  e:
  - MarkdownGenerator
  - CompactGenerator
  - JSONGenerator
  - YAMLGenerator
  - CSVGenerator
  c:
  - n: MarkdownGenerator
    d: Generates Markdown output for project analysis.
    m:
    - n: generate
      sig: project:ProjectInfo
      ret: str
      d: creates
    - n: _gen_tree
      sig: lines:List[str], project:ProjectInfo
      d: gen tree
    - n: _print_tree
      sig: lines:List[str], tree:dict, prefix:str
      d: logs tree
    - n: _gen_module
      sig: lines:List[str], m:ModuleInfo, detail:str, proj:ProjectInfo
      d: gen module
    - n: _gen_class
      sig: lines:List[str], cls:ClassInfo, detail:str
      d: gen class
    - n: _sig
      sig: f:FunctionInfo
      ret: str
      d: sig
  - n: CompactGenerator
    d: Generates ultra-compact output for token efficiency.
    m:
    - n: generate
      sig: project:ProjectInfo
      ret: str
      d: creates
  - n: JSONGenerator
    d: Generates JSON output for machine processing.
    m:
    - n: generate
      sig: project:ProjectInfo
      ret: str
      d: creates
    - n: generate_from_module
      sig: module:ModuleInfo
      ret: str
      d: creates from module
    - n: _generate_nested
      sig: project:ProjectInfo, detail:str
      ret: str
      d: creates nested
    - n: _generate_flat
      sig: project:ProjectInfo, detail:str
      ret: str
      d: creates flat
    - n: _build_element_row
      sig: m:ModuleInfo, elem_type:str, name:str, signature:str, f:FunctionInfo, ...+2
      ret: dict
      d: creates element row
    - n: _build_signature
      sig: f:FunctionInfo
      ret: str
      d: creates signature
    - n: _categorize
      sig: name:str
      ret: str
      d: categorize
    - n: _extract_domain
      sig: path:str
      ret: str
      d: parses domain
    - n: _compute_hash
      sig: name:str, signature:str
      ret: str
      d: processes hash
  - n: YAMLGenerator
    d: Generates YAML output for human-readable representation.
    m:
    - n: generate
      sig: project:ProjectInfo
      ret: str
      d: creates
    - n: generate_schema
      sig: ''
      ret: str
      d: creates schema
    - n: _generate_compact_schema
      sig: ''
      ret: str
      d: creates compact schema
    - n: _generate_full_schema
      sig: ''
      ret: str
      d: creates full schema
    - n: _generate_hybrid_schema
      sig: ''
      ret: str
      d: creates hybrid schema
    - n: generate_hybrid
      sig: project:ProjectInfo
      ret: str
      d: creates hybrid
    - n: _build_enhanced_signature
      sig: f:FunctionInfo
      ret: str
      d: creates enhanced signature
    - n: _extract_constants
      sig: module:ModuleInfo
      ret: list
      d: parses constants
    - n: _extract_dataclasses
      sig: module:ModuleInfo
      ret: list
      d: parses dataclasses
    - n: _extract_conditional_imports
      sig: module:ModuleInfo
      ret: list
      d: parses conditional imports
    - n: generate_from_module
      sig: module:ModuleInfo
      ret: str
      d: creates from module
    - n: _build_flat_data
      sig: project:ProjectInfo, detail:str
      ret: dict
      d: creates flat data
  - n: CSVGenerator
    d: Generates CSV output optimized for LLM processing.
    m:
    - n: generate
      sig: project:ProjectInfo
      ret: str
      d: creates
    - n: _build_row
      sig: m:ModuleInfo, elem_type:str, name:str, signature:str, calls:list, ...+2
      ret: dict
      d: creates row
    - n: _build_function_row
      sig: m:ModuleInfo, elem_type:str, name:str, f:FunctionInfo, deps:str, ...+2
      ret: dict
      d: creates function row
    - n: _build_signature
      sig: f:FunctionInfo
      ret: str
      d: creates signature
    - n: _categorize
      sig: name:str
      ret: str
      d: categorize
    - n: _extract_domain
      sig: path:str
      ret: str
      d: parses domain
    - n: _compute_hash
      sig: name:str, signature:str
      ret: str
      d: processes hash
    - n: _escape_csv
      sig: text:str
      ret: str
      d: escape csv
- p: markdown_format.py
  l: 266
  i:
  - os
  - dataclasses.dataclass
  - generators.YAMLGenerator
  - gherkin.GherkinGenerator
  - models.{ClassInfo,FunctionInfo,ModuleInfo,ProjectInfo}
  - pathlib.Path
  - typing.{Dict,List,Optional}
  e:
  - MarkdownSpec
  - MarkdownHybridGenerator
  - generate_markdown_hybrid
  - generate_file_markdown
  c:
  - n: MarkdownSpec
    d: Markdown specification for a project.
    props:
    - 'content: str'
    - 'file_count: int'
    - 'total_chars: int'
    - 'sections: Dict[str,int]'
  - n: MarkdownHybridGenerator
    d: Generates optimized Markdown hybrid format.
    m:
    - n: __init__
      sig: verbose:bool=False
      d: creates
    - n: generate
      sig: project:ProjectInfo, detail:str='full'
      ret: MarkdownSpec
      d: Generate Markdown hybrid specification.
    - n: _generate_header
      sig: project:ProjectInfo
      ret: str
      d: Generate header section.
    - n: _generate_tree
      sig: project:ProjectInfo
      ret: str
      d: Generate file tree section.
    - n: _generate_imports
      sig: project:ProjectInfo
      ret: str
      d: Generate imports as YAML for precise reproduction.
    - n: _generate_classes_yaml
      sig: project:ProjectInfo
      ret: str
      d: Generate classes as detailed YAML codeblock.
    - n: _generate_functions_gherkin
      sig: project:ProjectInfo
      ret: str
      d: Generate functions as detailed Gherkin codeblock.
    - n: _generate_dependencies
      sig: project:ProjectInfo
      ret: str
      d: Generate module dependencies section.
  f:
  - n: generate_markdown_hybrid
    sig: project:ProjectInfo, detail:str='full'
    ret: str
    d: Convenience function to generate Markdown hybrid f
  - n: generate_file_markdown
    sig: file_path:str
    ret: str
    d: Generate Markdown hybrid for a single file.
- p: models.py
  l: 292
  i:
  - dataclasses.{dataclass,field}
  - typing.{Dict,List,Optional}
- p: similarity.py
  l: 178
  i:
  - models.ModuleInfo
  - typing.{Dict,List}
  e:
  - SimilarityDetector
  - is_rapidfuzz_available
  - get_refactoring_suggestions
  c:
  - n: SimilarityDetector
    d: Detects similar functions using fuzzy string matching.
    m:
    - n: __init__
      sig: threshold:float=80.0
      d: Initialize the similarity detector.
    - n: find_similar_functions
      sig: modules:List[ModuleInfo]
      ret: Dict[str,List[str]]
      d: Find similar functions across all modules.
    - n: find_duplicate_signatures
      sig: modules:List[ModuleInfo]
      ret: Dict[str,List[str]]
      d: Find functions with identical signatures.
    - n: _build_signature
      sig: name:str, params:List[str], return_type:str=None
      ret: str
      d: Build a normalized signature string.
  f:
  - n: is_rapidfuzz_available
    sig: ''
    ret: bool
    d: Check if Rapidfuzz is available.
  - n: get_refactoring_suggestions
    sig: similar_functions:Dict[str,List[str]]
    ret: List[Dict[str,any]]
    d: Generate refactoring suggestions based on similar
  const:
  - n: RAPIDFUZZ_AVAILABLE
- p: universal.py
  l: 831
  i:
  - os
  - re
  - json
  - hashlib
  - datetime
  - dataclasses.{asdict,dataclass,field}
  - pathlib.Path
  - typing
  e:
  - ElementType
  - Language
  - Parameter
  - CodeElement
  - CodeLogic
  - UniversalParser
  - CodeGenerator
  - UniversalReproducer
  - reproduce_file
  c:
  - n: ElementType
    b:
    - Enum
    d: Types of code elements.
    props:
    - IMPORT
    - CLASS
    - INTERFACE
    - STRUCT
    - FUNCTION
    - METHOD
    - PROPERTY
    - CONSTANT
    - TYPE_ALIAS
    - ENUM
    - MODULE
  - n: Language
    b:
    - Enum
    d: Supported languages.
    props:
    - PYTHON
    - JAVASCRIPT
    - TYPESCRIPT
    - GO
    - RUST
    - JAVA
    - CSHARP
    - SQL
    - UNKNOWN
  - n: Parameter
    d: Function/method parameter.
    props:
    - 'name: str'
    - 'type: str'
    - 'default: str'
    - 'is_optional: bool'
  - n: CodeElement
    d: Universal representation of a code element.
    props:
    - 'type: ElementType'
    - 'name: str'
    - 'docstring: str'
    - 'signature: str'
    - 'parameters: List[Parameter]'
    - 'return_type: str'
    - 'body_hash: str'
    - 'attributes: List[Dict[str,str]]'
    - 'children: List[CodeElement]'
    - 'decorators: List[str]'
    - 'modifiers: List[str]'
    - 'extends: List[str]'
    - 'implements: List[str]'
  - n: CodeLogic
    d: Universal code logic representation for a single file.
    props:
    - 'source_file: str'
    - 'source_language: Language'
    - 'source_hash: str'
    - 'elements: List[CodeElement]'
    - 'imports: List[str]'
    - 'module_doc: str'
    - 'metadata: Dict[str,Any]'
    m:
    - n: to_dict
      sig: ''
      ret: Dict[str,Any]
      d: Convert to dictionary.
    - n: _element_to_dict
      sig: elem:CodeElement
      ret: Dict[str,Any]
      d: Convert element to dictionary.
    - n: to_compact
      sig: ''
      ret: str
      d: Convert to compact string representation.
    - n: _element_to_compact
      sig: elem:CodeElement, indent:int
      ret: List[str]
      d: Convert element to compact lines.
  - n: UniversalParser
    d: Parse source code into universal CodeLogic format.
    props:
    - LANG_PATTERNS
    m:
    - n: detect_language
      sig: content:str, file_ext:str
      ret: Language
      d: Detect programming language from content and exten
    - n: parse
      sig: file_path:Union[str,Path]
      ret: CodeLogic
      d: Parse source file into CodeLogic.
    - n: _parse_python
      sig: path:Path, content:str, hash_:str
      ret: CodeLogic
      d: Parse Python file.
    - n: _parse_js_ts
      sig: path:Path, content:str, hash_:str, lang:Language
      ret: CodeLogic
      d: Parse JavaScript/TypeScript file.
    - n: _parse_go
      sig: path:Path, content:str, hash_:str
      ret: CodeLogic
      d: Parse Go file.
    - n: _parse_sql
      sig: path:Path, content:str, hash_:str
      ret: CodeLogic
      d: Parse SQL file.
    - n: _parse_generic
      sig: path:Path, content:str, hash_:str, lang:Language
      ret: CodeLogic
      d: Generic parser for unknown languages.
  - n: CodeGenerator
    d: Generate code from CodeLogic in target language.
    m:
    - n: generate
      sig: logic:CodeLogic, target_lang:Language
      ret: str
      d: Generate code in target language.
    - n: _generate_python
      sig: logic:CodeLogic
      ret: str
      d: Generate Python code.
    - n: _generate_python_element
      sig: elem:CodeElement, indent:int=0
      ret: List[str]
      d: Generate Python code for element.
    - n: _generate_typescript
      sig: logic:CodeLogic
      ret: str
      d: Generate TypeScript code.
    - n: _generate_go
      sig: logic:CodeLogic
      ret: str
      d: Generate Go code.
    - n: _generate_sql
      sig: logic:CodeLogic
      ret: str
      d: Generate SQL code.
    - n: _generate_generic
      sig: logic:CodeLogic, target:Language
      ret: str
      d: Generate generic code.
  - n: UniversalReproducer
    d: Universal code reproduction system.
    m:
    - n: __init__
      sig: client:BaseLLMClient=None
      d: Initialize reproducer.
    - n: _get_client
      sig: ''
      ret: BaseLLMClient
      d: or create LLM client.
    - n: extract_logic
      sig: file_path:str
      ret: CodeLogic
      d: Extract code logic from file.
    - n: reproduce
      sig: source_path:str, target_lang:str=None, output_dir:str=None, use_llm:bool=True
      ret: Dict[str,Any]
      d: Reproduce code from source file.
    - n: _generate_with_llm
      sig: logic:CodeLogic, target:Language
      ret: str
      d: Generate code using LLM.
    - n: _save_result
      sig: output_dir:Path, original:str, logic:CodeLogic, generated:str, result:Dict[str,Any]
      d: Save reproduction results.
  f:
  - n: reproduce_file
    sig: source_path:str, target_lang:str=None, output_dir:str=None, use_llm:bool=True
    ret: Dict[str,Any]
    d: Convenience function for single file reproduction.
- p: benchmark.py
  l: 351
  i:
  - os
  - json
  - time
  - datetime
  - analyzer.analyze_project
  - dataclasses.{asdict,dataclass}
  - generators.{CompactGenerator,JSONGenerator,MarkdownGenerator}
  - pathlib.Path
  - typing.{Any,Dict,List,Optional}
  e:
  - FormatResult
  - BenchmarkResult
  - ReproductionBenchmark
  - run_benchmark
  c:
  - n: FormatResult
    d: Result for a single format test.
    props:
    - 'format_name: str'
    - 'spec_chars: int'
    - 'spec_tokens: int'
    - 'generated_chars: int'
    - 'similarity: float'
    - 'structural_score: float'
    - 'classes_match: bool'
    - 'functions_match: bool'
    - 'generation_time: float'
    - 'error: Optional[str]'
  - n: BenchmarkResult
    d: Complete benchmark result.
    props:
    - 'source_file: str'
    - 'source_chars: int'
    - 'source_classes: int'
    - 'source_functions: int'
    - 'timestamp: str'
    - 'model: str'
    - 'formats: List[FormatResult]'
    - 'best_format: str'
    - 'best_similarity: float'
  - n: ReproductionBenchmark
    d: Benchmark reproduction quality across formats.
    m:
    - n: __init__
      sig: client:BaseLLMClient=None
      d: Initialize benchmark.
    - n: generate_spec
      sig: file_path:Path, format_name:str, detail:str='full'
      ret: str
      d: Generate specification in given format.
    - n: reproduce_with_format
      sig: file_path:Path, format_name:str, original_code:str
      ret: FormatResult
      d: Test reproduction with a specific format.
    - n: run_single
      sig: file_path:str, formats:List[str]=None
      ret: BenchmarkResult
      d: Run benchmark on a single file.
    - n: run_all
      sig: files:List[str], output_dir:str=None
      ret: Dict[str,Any]
      d: Run benchmark on multiple files.
    - n: _generate_summary
      sig: results:List[BenchmarkResult]
      ret: Dict[str,Any]
      d: Generate summary from benchmark results.
    - n: _save_results
      sig: output_dir:Path, results:List[BenchmarkResult], summary:Dict
      d: Save benchmark results.
    - n: _generate_report
      sig: results:List[BenchmarkResult], summary:Dict
      ret: str
      d: Generate markdown benchmark report.
  f:
  - n: run_benchmark
    sig: files:List[str], output_dir:str='benchmark_results', provider:str=None, model:str=None
    ret: Dict[str,Any]
    d: Run reproduction benchmark.
  const:
  - n: FORMAT_PROMPTS
- p: terminal.py
  l: 500
  i:
  - os
  - re
  - sys
  - typing.{Any,Dict,List,Literal,Optional}
  e:
  - ShellRenderer
  - RenderAPI
  - get_renderer
  - set_renderer
  c:
  - n: ShellRenderer
    d: Renders colorized markdown output in terminal.
    m:
    - n: __init__
      sig: use_colors:bool=True, verbose:bool=True
      d: creates
    - n: _supports_colors
      sig: ''
      ret: bool
      d: Check if terminal supports ANSI colors.
    - n: enable_log
      sig: ''
      ret: None
      d: Enable log buffering for markdown export.
    - n: get_log
      sig: ''
      ret: str
      d: buffered log as clean markdown (no ANSI codes).
    - n: clear_log
      sig: ''
      ret: None
      d: Clear log buffer.
    - n: _log
      sig: text:str
      ret: None
      d: Log a line (strips ANSI codes for markdown).
    - n: _c
      sig: color:str, text:str
      ret: str
      d: Apply color to text.
    - n: heading
      sig: level:int, text:str
      ret: None
      d: Print a markdown heading.
    - n: codeblock
      sig: language:Language, content:str
      ret: None
      d: Print a syntax-highlighted code block.
    - n: render_markdown
      sig: text:str
      ret: None
      d: Render full markdown text with syntax highlighting
    - n: success
      sig: message:str
      ret: None
      d: Print success message.
    - n: error
      sig: message:str
      ret: None
      d: Print error message.
  - n: RenderAPI
    d: Convenience API for terminal rendering.
    m:
    - n: heading
      sig: level:int, text:str
      ret: None
      d: heading
    - n: code
      sig: lang:Language, content:str
      ret: None
      d: code
    - n: codeblock
      sig: lang:Language, content:str
      ret: None
      d: codeblock
    - n: markdown
      sig: text:str
      ret: None
      d: markdown
    - n: success
      sig: message:str
      ret: None
      d: success
    - n: error
      sig: message:str
      ret: None
      d: error
    - n: warning
      sig: message:str
      ret: None
      d: warning
    - n: info
      sig: message:str
      ret: None
      d: info
    - n: status
      sig: icon:str, message:str, type:Literal[info,success,warning,error]='info'
      ret: None
      d: status
    - n: kv
      sig: key:str, value:Any
      ret: None
      d: kv
    - n: progress
      sig: done:int, total:int, label:str=''
      ret: None
      d: progress
    - n: separator
      sig: char:str='', width:int=60
      ret: None
      d: separator
  f:
  - n: get_renderer
    sig: use_colors:bool=True, verbose:bool=True
    ret: ShellRenderer
    d: or create the global renderer instance.
  - n: set_renderer
    sig: renderer:ShellRenderer
    ret: None
    d: the global renderer instance.
  const:
  - n: COLORS
- p: toon_format.py
  l: 477
  i:
  - re
  - models.{ClassInfo,FunctionInfo,ModuleInfo,ProjectInfo}
  - shared_utils.{compact_imports,truncate_docstring}
  - typing.{Any,Dict,List,Optional}
  e:
  - TOONGenerator
  - TOONParser
  - generate_toon
  - parse_toon
  c:
  - n: TOONGenerator
    d: Generates TOON format output from ProjectInfo.
    props:
    - SPECIAL_CHARS
    - LOOKS_LIKE_LITERAL
    m:
    - n: __init__
      sig: delimiter:str=',', use_tabs:bool=False
      d: Initialize TOON generator.
    - n: generate
      sig: project:ProjectInfo, detail:str='standard'
      ret: str
      d: Generate TOON format from ProjectInfo.
    - n: _generate_modules
      sig: modules:List[ModuleInfo], detail:str
      ret: List[str]
      d: Generate modules section.
    - n: _generate_classes
      sig: classes:List[ClassInfo], detail:str, indent:int=0
      ret: List[str]
      d: Generate classes in TOON format.
    - n: _generate_methods
      sig: methods:List[FunctionInfo], detail:str='standard', indent:int=0
      ret: List[str]
      d: Generate methods in tabular TOON format.
    - n: _generate_functions
      sig: functions:List[FunctionInfo], detail:str, indent:int=0
      ret: List[str]
      d: Generate functions in tabular TOON format.
    - n: _build_signature
      sig: f:FunctionInfo
      ret: str
      d: Build compact signature string without self/cls.
    - n: _quote
      sig: value:Any
      ret: str
      d: Quote a value if necessary for TOON format.
    - n: generate_compact
      sig: project:ProjectInfo
      ret: str
      d: Generate minimal TOON output.
    - n: generate_full
      sig: project:ProjectInfo
      ret: str
      d: Generate detailed TOON output.
    - n: generate_schema
      sig: format_type:str='standard'
      ret: str
      d: Generate JSON Schema for the TOON format.
    - n: generate_ultra_compact
      sig: project:ProjectInfo
      ret: str
      d: Generate minimal TOON with abbreviated keys.
  - n: TOONParser
    d: Parse TOON format back to Python dict.
    m:
    - n: __init__
      sig: ''
      d: creates
    - n: parse
      sig: content:str
      ret: Dict[str,Any]
      d: Parse TOON content to dict.
    - n: _parse_value
      sig: value:str
      ret: Any
      d: Parse a TOON value to Python type.
  f:
  - n: generate_toon
    sig: project:ProjectInfo, detail:str='standard', use_tabs:bool=False
    ret: str
    d: Convenience function to generate TOON format.
  - n: parse_toon
    sig: content:str
    ret: Dict[str,Any]
    d: Convenience function to parse TOON content.
- p: dependency.py
  l: 187
  i:
  - models.{DependencyNode,ModuleInfo}
  - pathlib.Path
  - typing.{Dict,List,Optional}
  e:
  - DependencyAnalyzer
  - is_networkx_available
  c:
  - n: DependencyAnalyzer
    d: Analyzes dependency graphs using NetworkX.
    m:
    - n: __init__
      sig: ''
      d: Initialize the dependency analyzer.
    - n: build_graph
      sig: modules:List[ModuleInfo]
      ret: Dict[str,List[str]]
      d: Build dependency graph from modules.
    - n: analyze_metrics
      sig: ''
      ret: Dict[str,DependencyNode]
      d: Compute metrics for each node in the graph.
    - n: get_entrypoints
      sig: ''
      ret: List[str]
      d: entry points (nodes with no incoming edges).
    - n: get_hubs
      sig: ''
      ret: List[str]
      d: hub modules (high centrality).
    - n: detect_cycles
      sig: ''
      ret: List[List[str]]
      d: Detect dependency cycles.
    - n: get_strongly_connected_components
      sig: ''
      ret: List[List[str]]
      d: strongly connected components.
    - n: _detect_clusters
      sig: ''
      ret: Dict[str,int]
      d: Detect clusters using connected components.
    - n: _module_name
      sig: path:str
      ret: str
      d: Convert file path to module name.
    - n: get_dependency_depth
      sig: module_path:str
      ret: int
      d: the maximum depth of dependencies for a module.
  f:
  - n: is_networkx_available
    sig: ''
    ret: bool
    d: Check if NetworkX is available.
  const:
  - n: NETWORKX_AVAILABLE
- p: mcp_server.py
  l: 293
  i:
  - json
  - sys
  - __version__
  - pathlib.Path
  - typing.Optional
  e:
  - handle_request
  - call_tool
  - run_server
  f:
  - n: handle_request
    sig: request:dict
    ret: dict
    d: Handle incoming MCP request.
  - n: call_tool
    sig: tool_name:str, arguments:dict
    ret: str
    d: Execute a tool and return result.
  - n: run_server
    sig: ''
    d: Run the MCP server.
- p: reproduction.py
  l: 333
  i:
  - re
  - difflib
  - datetime
  - llm_clients.{BaseLLMClient,get_client}
  - pathlib.Path
  - typing.{Any,Dict,List,Optional}
  e:
  - CodeReproducer
  - generate_file_gherkin
  - compare_code
  - extract_code_block
  c:
  - n: CodeReproducer
    d: Code reproduction workflow using LLM.
    m:
    - n: __init__
      sig: client:BaseLLMClient=None, provider:str=None
      d: Initialize reproducer.
    - n: reproduce_file
      sig: source_path:str, output_dir:str=None
      ret: Dict[str,Any]
      d: Reproduce code from a source file.
    - n: generate_from_gherkin
      sig: gherkin:str, language:str='python'
      ret: str
      d: Generate code from Gherkin specification.
    - n: _save_results
      sig: output_dir:Path, results:Dict[str,Any]
      d: Save reproduction results to files.
    - n: _generate_report
      sig: results:Dict[str,Any]
      ret: str
      d: Generate markdown comparison report.
  f:
  - n: generate_file_gherkin
    sig: file_path:Path
    ret: str
    d: Generate detailed Gherkin specification for a sing
  - n: compare_code
    sig: original:str, generated:str
    ret: Dict[str,Any]
    d: Compare original and generated code.
  - n: extract_code_block
    sig: text:str, language:str='python'
    ret: str
    d: Extract code block from LLM response.
- p: gherkin.py
  l: 766
  i:
  - re
  - hashlib
  - collections.defaultdict
  - dataclasses.{dataclass,field}
  - models.{ClassInfo,FunctionInfo,ModuleInfo,ProjectInfo}
  - typing.{Any,Dict,List,Optional,Set}
  e:
  - GherkinScenario
  - GherkinFeature
  - StepDefinition
  - GherkinGenerator
  - StepDefinitionGenerator
  - CucumberYAMLGenerator
  - csv_to_gherkin
  - gherkin_to_test_data
  c:
  - n: GherkinScenario
    d: Represents a single Gherkin scenario.
    props:
    - 'name: str'
    - 'given: List[str]'
    - 'when: List[str]'
    - 'then: List[str]'
    - 'tags: List[str]'
    - 'examples: Optional[List[Dict[str,str]]]'
    - 'data_table: Optional[List[Dict[str,str]]]'
  - n: GherkinFeature
    d: Represents a Gherkin feature file.
    props:
    - 'name: str'
    - 'description: str'
    - 'tags: List[str]'
    - 'scenarios: List[GherkinScenario]'
    - 'background: Optional[List[str]]'
    - 'rules: Optional[List[Dict[str,Any]]]'
  - n: StepDefinition
    d: Represents a step definition.
    props:
    - 'pattern: str'
    - 'step_type: str'
    - 'function_name: str'
    - 'params: List[str]'
    - 'implementation_hint: str'
  - n: GherkinGenerator
    d: Generates Gherkin feature files from code analysis.
    props:
    - CATEGORY_VERBS
    - DOMAIN_CONTEXTS
    - KEYWORDS
    m:
    - n: __init__
      sig: language:str='en'
      d: Initialize GherkinGenerator.
    - n: generate
      sig: project:ProjectInfo, detail:str='standard', group_by:str='domain'
      ret: str
      d: Generate Gherkin feature files from project analys
    - n: generate_test_scenarios
      sig: project:ProjectInfo, group_by:str='domain'
      ret: List[GherkinFeature]
      d: Generate structured test scenarios for programmati
    - n: get_step_definitions
      sig: ''
      ret: List[StepDefinition]
      d: all unique step definitions from generated feature
    - n: _extract_features
      sig: project:ProjectInfo, group_by:str
      ret: List[GherkinFeature]
      d: Extract Gherkin features from project.
    - n: _create_feature
      sig: group_name:str, items:List[dict], project:ProjectInfo, group_by:str
      ret: GherkinFeature
      d: a Gherkin feature from grouped items.
    - n: _create_scenario
      sig: category:str, items:List[dict], domain:str
      ret: GherkinScenario
      d: a scenario from category items.
    - n: _create_edge_case_scenarios
      sig: category:str, items:List[dict]
      ret: List[GherkinScenario]
      d: edge case scenarios for thorough testing.
    - n: _create_when_step
      sig: func:FunctionInfo, verb:str
      ret: str
      d: a When step from function info.
    - n: _create_background
      sig: domain:str, items:List[dict]
      ret: Optional[List[str]]
      d: background steps for common setup.
    - n: _create_examples_table
      sig: items:List[dict]
      ret: List[Dict[str,str]]
      d: Examples table for Scenario Outline.
    - n: _extract_param_placeholders
      sig: func:FunctionInfo
      ret: str
      d: Extract parameter placeholders for Gherkin steps.
  - n: StepDefinitionGenerator
    d: Generates step definition stubs from Gherkin features.
    m:
    - n: generate_pytest_bdd
      sig: features:List[GherkinFeature]
      ret: str
      d: Generate pytest-bdd step definitions.
    - n: generate_behave
      sig: features:List[GherkinFeature]
      ret: str
      d: Generate behave step definitions.
    - n: generate_cucumber_js
      sig: features:List[GherkinFeature]
      ret: str
      d: Generate Cucumber.js step definitions.
    - n: _step_to_func_name
      sig: step:str
      ret: str
      d: Convert step text to valid function name.
  - n: CucumberYAMLGenerator
    d: Generates Cucumber YAML configuration and test data.
    m:
    - n: generate
      sig: project:ProjectInfo, detail:str='standard'
      ret: str
      d: Generate Cucumber YAML configuration.
    - n: _extract_domain
      sig: path:str
      ret: str
      d: Extract domain from path.
    - n: _categorize
      sig: name:str
      ret: str
      d: Categorize by name pattern.
  f:
  - n: csv_to_gherkin
    sig: csv_content:str, language:str='en'
    ret: str
    d: Convert CSV analysis directly to Gherkin.
  - n: gherkin_to_test_data
    sig: gherkin_content:str
    ret: Dict[str,Any]
    d: Extract structured test data from Gherkin for LLM
- p: schemas/logicml_schema.py
  l: 190
  i:
  - re
  - dataclasses.{dataclass,field}
  - typing.{Any,Dict,List,Optional,Tuple}
  e:
  - LogicMLMethod
  - LogicMLClass
  - LogicMLModule
  - LogicMLSchema
  - validate_logicml
  - parse_logicml_header
  - extract_logicml_signature
  c:
  - n: LogicMLMethod
    d: Schema for LogicML method.
    props:
    - 'name: str'
    - 'signature: str'
    - 'does: str'
    - 'edge: List[str]'
    - 'side: str'
    - 'is_async: bool'
    - 'is_property: bool'
  - n: LogicMLClass
    d: Schema for LogicML class.
    props:
    - 'name: str'
    - 'doc: str'
    - 'bases: List[str]'
    - 'attrs: Dict[str,str]'
    - 'methods: List[LogicMLMethod]'
    - 'is_pydantic: bool'
    - 'is_enum: bool'
    - 'is_dataclass: bool'
  - n: LogicMLModule
    d: Schema for LogicML module.
    props:
    - 'filename: str'
    - 'lines: int'
    - 'classes: List[str]'
    - 'imports: Dict[str,List[str]]'
    - 'module_classes: List[LogicMLClass]'
    - 'functions: List[LogicMLMethod]'
    - 'module_type: str'
    - 'exports: List[str]'
  - n: LogicMLSchema
    d: Complete LogicML specification schema.
    props:
    - 'modules: List[LogicMLModule]'
    - 'token_estimate: int'
    - 'file_count: int'
    - 'class_count: int'
    - 'function_count: int'
  f:
  - n: validate_logicml
    sig: spec:str
    ret: Tuple[bool,List[str]]
    d: Validate LogicML specification.
  - n: parse_logicml_header
    sig: line:str
    ret: Optional[Dict[str,Any]]
    d: Parse LogicML header comment.
  - n: extract_logicml_signature
    sig: sig_line:str
    ret: Dict[str,Any]
    d: 'Extract signature components from LogicML sig: lin'
- p: schemas/__init__.py
  l: 25
  i:
  - json_schema.{JSONSchema,parse_json_spec,validate_json}
  - logicml_schema.{LogicMLSchema,validate_logicml}
  - markdown_schema.{MarkdownSchema,validate_markdown}
  - yaml_schema.{YAMLSchema,validate_yaml}
- p: schemas/yaml_schema.py
  l: 164
  i:
  - yaml
  - dataclasses.{dataclass,field}
  - typing.{Any,Dict,List,Optional,Tuple}
  e:
  - MethodSchema
  - ClassSchema
  - FunctionSchema
  - ModuleSchema
  - YAMLSchema
  - validate_yaml
  c:
  - n: MethodSchema
    d: Schema for method definition.
    props:
    - 'name: str'
    - 'signature: str'
    - 'intent: str'
    - 'lines: int'
    - 'is_async: bool'
    - 'decorators: List[str]'
    - 'raises: List[str]'
  - n: ClassSchema
    d: Schema for class definition.
    props:
    - 'name: str'
    - 'bases: List[str]'
    - 'docstring: str'
    - 'methods: List[MethodSchema]'
    - 'properties: List[str]'
    - 'is_abstract: bool'
    - 'is_dataclass: bool'
  - n: FunctionSchema
    d: Schema for function definition.
    props:
    - 'name: str'
    - 'signature: str'
    - 'intent: str'
    - 'lines: int'
    - 'is_async: bool'
    - 'decorators: List[str]'
  - n: ModuleSchema
    d: Schema for module definition.
    props:
    - 'path: str'
    - 'language: str'
    - 'lines: int'
    - 'imports: List[str]'
    - 'exports: List[str]'
    - 'classes: List[ClassSchema]'
    - 'functions: List[FunctionSchema]'
  - n: YAMLSchema
    d: Complete YAML specification schema.
    props:
    - 'project: str'
    - 'statistics: Dict[str,Any]'
    - 'modules: List[ModuleSchema]'
  f:
  - n: validate_yaml
    sig: spec:str
    ret: Tuple[bool,List[str]]
    d: Validate YAML specification.
  - n: _validate_module
    sig: module:Dict, index:int
    ret: List[str]
    d: Validate a module definition.
  - n: _validate_class
    sig: cls:Dict, prefix:str
    ret: List[str]
    d: Validate a class definition.
- p: schemas/json_schema.py
  l: 206
  i:
  - json
  - dataclasses.{dataclass,field}
  - typing.{Any,Dict,List,Optional,Tuple}
  e:
  - JSONMethodSchema
  - JSONClassSchema
  - JSONFunctionSchema
  - JSONModuleSchema
  - JSONSchema
  - validate_json
  - parse_json_spec
  c:
  - n: JSONMethodSchema
    d: Schema for JSON method definition.
    props:
    - 'name: str'
    - 'signature: str'
    - 'intent: str'
    - 'is_async: bool'
    - 'decorators: List[str]'
    - 'params: List[str]'
    - 'return_type: str'
  - n: JSONClassSchema
    d: Schema for JSON class definition.
    props:
    - 'name: str'
    - 'bases: List[str]'
    - 'docstring: str'
    - 'methods: List[JSONMethodSchema]'
    - 'properties: Dict[str,str]'
    - 'is_abstract: bool'
    - 'is_dataclass: bool'
  - n: JSONFunctionSchema
    d: Schema for JSON function definition.
    props:
    - 'name: str'
    - 'signature: str'
    - 'intent: str'
    - 'is_async: bool'
    - 'params: List[str]'
    - 'return_type: str'
  - n: JSONModuleSchema
    d: Schema for JSON module definition.
    props:
    - 'path: str'
    - 'language: str'
    - 'lines: int'
    - 'imports: List[str]'
    - 'exports: List[str]'
    - 'classes: List[JSONClassSchema]'
    - 'functions: List[JSONFunctionSchema]'
  - n: JSONSchema
    d: Complete JSON specification schema.
    props:
    - 'project: str'
    - 'statistics: Dict[str,Any]'
    - 'modules: List[JSONModuleSchema]'
  f:
  - n: validate_json
    sig: spec:str
    ret: Tuple[bool,List[str]]
    d: Validate JSON specification.
  - n: _validate_json_module
    sig: module:Dict, index:int
    ret: List[str]
    d: Validate a JSON module definition.
  - n: _validate_json_class
    sig: cls:Dict, prefix:str
    ret: List[str]
    d: Validate a JSON class definition.
  - n: parse_json_spec
    sig: spec:str
    ret: Optional[JSONSchema]
    d: Parse JSON specification into schema.
- p: schemas/markdown_schema.py
  l: 121
  i:
  - re
  - dataclasses.{dataclass,field}
  - typing.{Any,Dict,List,Optional,Tuple}
  e:
  - MarkdownMethod
  - MarkdownClass
  - MarkdownModule
  - MarkdownSchema
  - validate_markdown
  - extract_markdown_sections
  c:
  - n: MarkdownMethod
    d: Schema for Markdown method.
    props:
    - 'name: str'
    - 'signature: str'
    - 'is_async: bool'
    - 'gherkin_scenarios: List[str]'
  - n: MarkdownClass
    d: Schema for Markdown class.
    props:
    - 'name: str'
    - 'bases: List[str]'
    - 'attributes: Dict[str,str]'
    - 'methods: List[MarkdownMethod]'
  - n: MarkdownModule
    d: Schema for Markdown module.
    props:
    - 'filename: str'
    - 'language: str'
    - 'lines: int'
    - 'imports: List[str]'
    - 'classes: List[MarkdownClass]'
    - 'functions: List[MarkdownMethod]'
  - n: MarkdownSchema
    d: Complete Markdown specification schema.
    props:
    - 'modules: List[MarkdownModule]'
    - 'token_estimate: int'
  f:
  - n: validate_markdown
    sig: spec:str
    ret: Tuple[bool,List[str]]
    d: Validate Markdown specification.
  - n: extract_markdown_sections
    sig: spec:str
    ret: Dict[str,Any]
    d: Extract sections from Markdown specification.
- p: benchmarks/common.py
  l: 206
  i:
  - datetime
  - json
  - __future__.annotations
  - generators.{JSONGenerator,YAMLGenerator}
  - gherkin.GherkinGenerator
  - logicml.LogicMLGenerator
  - markdown_format.MarkdownHybridGenerator
  - models.ProjectInfo
  - pathlib.Path
  - toon_format.TOONGenerator
  e:
  - create_single_project
  - generate_spec
  - generate_spec_token
  - get_async_reproduction_prompt
  - get_token_reproduction_prompt
  - get_simple_reproduction_prompt
  f:
  - n: create_single_project
    sig: module_info, file_path:Path
    ret: ProjectInfo
    d: creates single project
  - n: generate_spec
    sig: project:ProjectInfo, fmt:str
    ret: str
    d: creates spec
  - n: _generate_token_json
    sig: project:ProjectInfo
    ret: str
    d: Generate compact, token-friendly JSON spec (used b
  - n: _generate_token_json_compact
    sig: project:ProjectInfo
    ret: str
    d: creates token json compact
  - n: generate_spec_token
    sig: project:ProjectInfo, fmt:str
    ret: str
    d: Generate spec optimized for token benchmark (keeps
  - n: get_async_reproduction_prompt
    sig: spec:str, fmt:str, file_name:str, with_tests:bool=False
    ret: str
    d: retrieves async reproduction prompt
  - n: get_token_reproduction_prompt
    sig: spec:str, fmt:str, file_name:str
    ret: str
    d: retrieves token reproduction prompt
  - n: get_simple_reproduction_prompt
    sig: spec:str, fmt:str, file_name:str
    ret: str
    d: retrieves simple reproduction prompt
- p: benchmarks/runner.py
  l: 638
  i:
  - re
  - sys
  - time
  - analyzer.analyze_project
  - concurrent.futures.{ThreadPoolExecutor,as_completed}
  - dataclasses.dataclass
  - llm_clients.{BaseLLMClient,get_client}
  - pathlib.Path
  - typing.{Callable,Dict,List,Optional,Tuple}
  e:
  - BenchmarkRunner
  - run_benchmark
  c:
  - n: BenchmarkRunner
    d: Unified benchmark runner for code2logic.
    m:
    - n: __init__
      sig: client:Optional[BaseLLMClient]=None, config:Optional[BenchmarkConfig]=None
      d: Initialize benchmark runner.
    - n: _should_use_llm
      sig: ''
      ret: bool
      d: whether this runner should call an LLM.
    - n: _get_client
      sig: ''
      ret: BaseLLMClient
      d: or create LLM client.
    - n: _template_generate_code
      sig: spec:str, fmt:str, file_name:str
      ret: str
      d: Generate minimal Python code without an LLM (fallb
    - n: run_format_benchmark
      sig: folder:str, formats:List[str]=None, limit:Optional[int]=None, verbose:bool=False
      ret: BenchmarkResult
      d: Run format comparison benchmark.
    - n: _test_format
      sig: project, original:str, fmt:str, file_name:str, client:Optional[BaseLLMClient], ...+1
      ret: FormatResult
      d: Test a single format.
    - n: run_file_benchmark
      sig: file_path:str, formats:List[str]=None, verbose:bool=False
      ret: BenchmarkResult
      d: Run benchmark on a single file.
    - n: run_function_benchmark
      sig: file_path:str, function_names:List[str]=None, limit:Optional[int]=None, verbose:bool=False
      ret: BenchmarkResult
      d: Run function-level reproduction benchmark.
    - n: _test_function
      sig: func, content:str, language:str, file_path:Path, client:Optional[BaseLLMClient], ...+1
      ret: FunctionResult
      d: Test reproduction of a single function.
    - n: run_project_benchmark
      sig: project_path:str, formats:List[str]=None, limit:Optional[int]=None, verbose:bool=False
      ret: BenchmarkResult
      d: Run benchmark on entire project.
    - n: _reproduce_module
      sig: module_info, fmt:str, project_root:str, client:Optional[BaseLLMClient], verbose:bool=False
      ret: FileResult
      d: Reproduce a single module.
  f:
  - n: _test_python_syntax
    sig: code:str
    ret: bool
    d: Test if Python code has valid syntax.
  - n: _test_python_runs
    sig: code:str, timeout:int=5
    ret: bool
    d: Test if Python code runs without errors.
  - n: _extract_code
    sig: response:str
    ret: str
    d: Extract code from LLM response.
  - n: run_benchmark
    sig: source:str, benchmark_type:str='format', formats:List[str]=None, limit:Optional[int]=None, output:Optional[str]=None,
      verbose:bool=False
    ret: BenchmarkResult
    d: Convenience function to run benchmarks.
- p: benchmarks/__init__.py
  l: 19
  i:
  - common
  - results.{BenchmarkConfig,BenchmarkResult,FileResult,FormatResult,FunctionResult}
  - runner.{BenchmarkRunner,run_benchmark}
- p: benchmarks/results.py
  l: 148
  i:
  - datetime
  - json
  - dataclasses.{asdict,dataclass,field}
  - pathlib.Path
  - typing.{Any,Dict,List,Optional}
- p: core/__init__.py
  l: 20
  i:
  - analyzer.{ProjectAnalyzer,analyze_project}
  - dependency.DependencyAnalyzer
  - errors
  - models
- p: formats/__init__.py
  l: 27
  i:
  - generators.{CSVGenerator,CompactGenerator,JSONGenerator,MarkdownGenerator,YAMLGenerator}
  - gherkin.{CucumberYAMLGenerator,GherkinGenerator,StepDefinitionGenerator,csv_to_gherkin,gherkin_to_test_data}
  - logicml.{LogicMLGenerator,LogicMLSpec}
  - markdown_format.{MarkdownHybridGenerator,MarkdownSpec}
  - toon_format.TOONGenerator
- p: tools/__init__.py
  l: 21
  i:
  - benchmark.{BenchmarkResult,FormatResult,ReproductionBenchmark,run_benchmark}
  - code_review.{CodeReviewer,analyze_code_quality,check_performance_issues,check_security_issues}
  - refactor
- p: llm/__init__.py
  l: 13
  i:
  - intent.EnhancedIntentGenerator
  - llm_clients.{BaseLLMClient,LiteLLMClient,OllamaLocalClient,OpenRouterClient,get_client}
- p: integrations/__init__.py
  l: 5
  i:
  - mcp_server.{call_tool,handle_request,run_server}
