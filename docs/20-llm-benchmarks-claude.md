# LLM Benchmarks + Claude (Anthropic)

[← Docs Index](00-index.md) | [← Benchmarking](10-benchmark.md) | [← LLM Integration](08-llm-integration.md)

This document focuses on running Code2Logic benchmarks **with an LLM enabled**, including how to force **Claude (Anthropic)** via provider/model selection.

## What benchmarks measure (important)

- **Format / project benchmarks** measure **reproduction quality from a spec** (structure + syntax + similarity heuristics).
- **High scores are not proof of runtime equivalence.** Runtime equivalence is validated only by tests / behavioral checks.
- `--no-llm` is a **pipeline/sanity mode** (template fallback), not meaningful for comparing LLM quality.

## Key artifacts

### `project.toon`

Project-level TOON (structure of modules/classes/functions). Good for “big picture”.

### `function.toon`

Function-logic TOON (detailed per-function index). In this repo, `function.toon` is generated by:

```bash
code2logic ./ -f toon --compact --no-repeat-module \
  --function-logic function.toon --with-schema --name project -o ./
```

Schema (optional): `function-schema.json`.

## Quickstart: run the repo benchmarks

### Offline (no API calls)

```bash
make benchmark BENCH_USE_LLM=0
```

### With LLM enabled

```bash
make benchmark BENCH_USE_LLM=1
```

Notes:

- `BENCH_USE_LLM=1` requires at least one configured provider (see `08-llm-integration.md`).
- Output artifacts are written to `examples/output/`.

## Force Claude (Anthropic)

You can use Claude in two common ways:

### Option A: Claude via OpenRouter

Requirements:

- `OPENROUTER_API_KEY=...`

Example (format benchmark):

```bash
python examples/15_unified_benchmark.py \
  --type format \
  --folder tests/samples/ \
  --formats yaml toon logicml json markdown csv gherkin function.toon \
  --limit 20 --verbose \
  --provider openrouter \
  --model anthropic/claude-3.5-sonnet
```

### Option B: Claude via Anthropic API (through LiteLLM)

Requirements:

- `ANTHROPIC_API_KEY=...`

Example (project benchmark):

```bash
python examples/15_unified_benchmark.py \
  --type project \
  --folder tests/samples/ \
  --formats yaml toon logicml json markdown csv gherkin function.toon \
  --limit 20 --verbose \
  --provider litellm \
  --model anthropic/claude-3.5-sonnet
```

## Speed & cost knobs

### Concurrency

Use fewer workers if you hit rate limits:

```bash
python examples/15_unified_benchmark.py --type format --workers 2
```

### Output token limit

```bash
python examples/15_unified_benchmark.py --type format --max-tokens 2500
```

Guidance:

- Lower `--max-tokens` reduces cost and latency, but may reduce reproduction quality.
- Increase `--max-tokens` for larger files/specs, but expect slower runs.

## Troubleshooting

- **No provider available**: configure keys/models in `.env` or via `code2logic llm ...` (see `08-llm-integration.md`).
- **Rate limited**: reduce `--workers`, consider cheaper/faster model (e.g. Haiku), or switch provider.
- **Weird output (explanations instead of code)**: use stricter prompts or lower temperature on provider side; benchmark runner already tries to extract fenced code blocks.
