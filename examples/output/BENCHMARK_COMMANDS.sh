# Auto-generated by make benchmark
set -euo pipefail

poetry run python examples/15_unified_benchmark.py  --type format --folder tests/samples/ --formats yaml toon logicml json markdown csv gherkin function.toon --limit 20 --verbose --output examples/output/benchmark_format.json
poetry run python examples/15_unified_benchmark.py  --type function --file tests/samples/sample_functions.py --limit 10 --verbose --output examples/output/benchmark_function.json
poetry run python examples/behavioral_benchmark.py
poetry run python examples/11_token_benchmark.py  --folder tests/samples/ --formats yaml toon logicml json markdown csv gherkin function.toon --limit 20 --verbose --output examples/output/benchmark_token.json
poetry run python examples/15_unified_benchmark.py  --type project --folder tests/samples/ --formats yaml toon logicml json markdown csv gherkin function.toon --limit 20 --verbose --output examples/output/benchmark_project.json
poetry run python -m code2logic ./ -f toon --compact --name project -o ./
poetry run python -m code2logic ./ -f toon --compact --no-repeat-module --function-logic function.toon --with-schema --name project -o ./
poetry run python -m code2logic ./ -f yaml --compact --name project -o examples/output/
poetry run python -m code2logic ./ -f json --name project -o examples/output/
poetry run python -m code2logic ./ -f markdown --name project -o examples/output/
poetry run python -m code2logic ./ -f compact --name project -o examples/output/
poetry run python -m code2logic ./ -f csv -d standard --name project -o examples/output/
