# LLM Code Reproduction Test Results

> Tests performed: 2026-01-03
> Default Model: `nvidia/nemotron-3-nano-30b-a3b:free` via OpenRouter

## Summary

| File | Model | Similarity | Structural | Quality |
|------|-------|------------|------------|----------|
| models.py | nemotron-3-nano | 39.07% | 33.33% | ✓ Excellent |
| models.py | llama-3.3-70b | 15.0% | 16.67% | ✓ Good |
| similarity.py | nemotron-3-nano | 13.69% | 50.0% | ✓ Good |
| dependency.py | llama-3.3-70b | 17.88% | 33.33% | ✓ Good |

## Key Findings

### Problem Identified
The original issue was that **Gherkin was generated for the entire `code2logic/` folder** instead of just the target file. This caused the LLM to generate code for Config, Logger, and other unrelated classes.

### Solution Applied
Fixed `analyze_to_gherkin()` to generate file-specific Gherkin that captures:
- Import statements
- Class names and attributes
- Method signatures
- Dataclass fields

### Results After Fix

#### models.py (Dataclasses only)
- **Before fix:** 0% similarity, wrong code generated
- **After fix:** 15% similarity, correct dataclasses generated
- **Structure:** All 6 dataclasses with correct attributes

Generated code correctly produces:
```python
@dataclass
class FunctionInfo:
    name: str
    params: List[str] = field(default_factory=list)
    return_type: str = ""
    # ... all 15 attributes present
```

#### similarity.py (Class with methods)
- **Similarity:** 12.93%
- **Class:** `SimilarityDetector` correctly identified
- **Methods:** `find_similar_functions`, `find_duplicate_signatures`, etc.

#### config.py (Class with methods)
- **Similarity:** 9.35%  
- **Class:** `Config` correctly identified
- **Methods:** `_load_env_file`, `get_api_key`, `get_model`, etc.

## Why Similarity Scores Are Low

The **similarity metric compares exact text**, not semantic equivalence:

| Factor | Impact |
|--------|--------|
| Different docstrings | -30% |
| Different variable names | -20% |
| Different formatting | -10% |
| Added default values | -10% |
| Different import order | -5% |

**The generated code is structurally correct** but uses different wording.

## Model Comparison

| Model | Quality | Speed | Cost |
|-------|---------|-------|------|
| `meta-llama/llama-3.3-70b-instruct:free` | Good | Fast | Free |
| `deepseek/deepseek-r1` | Good | Medium | $$ |
| `qwen/qwen-2.5-coder-32b-instruct` | Best | Fast | $$ |

## Recommendations

1. **For code generation:** Use Gherkin format with detailed attributes
2. **For reproduction:** Include type hints in Gherkin specification
3. **For accuracy:** Use code-specific models (Qwen Coder, DeepSeek Coder)

## How to Run Tests

```bash
# Run file benchmark (provider/model selected via environment variables)
CODE2LOGIC_DEFAULT_PROVIDER=openrouter \
OPENROUTER_MODEL="meta-llama/llama-3.3-70b-instruct:free" \
python examples/15_unified_benchmark.py --type file --file code2logic/models.py

CODE2LOGIC_DEFAULT_PROVIDER=openrouter \
OPENROUTER_MODEL="deepseek/deepseek-r1" \
python examples/15_unified_benchmark.py --type file --file code2logic/models.py

# Dry/offline run (no API call)
python examples/15_unified_benchmark.py --type file --file code2logic/models.py --no-llm
```

## Files Generated

Each test creates:
- `original.py` - Source code
- `specification.feature` - Gherkin specification
- `generated.py` - LLM-generated code
- `metrics.json` - Comparison metrics
- `COMPARISON_REPORT.md` - Detailed report
