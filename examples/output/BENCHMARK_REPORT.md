# Benchmark Report

> Generated: 2026-02-26 20:35:25
> Output dir: `/home/tom/github/wronai/code2logic/examples/output`

## Summary

| Benchmark | Items | Score/Similarity | Syntax OK | Runs OK | Fail% | Best |
|---|---:|---:|---:|---:|---:|---|
| Format | 20 files | 71.3% | 100% | 50% | 0% | json (64.3%) |
| Function-logic format | 20 files | 45.5% | 90% | 50% | 0% | function.toon (45.5%) |
| Token | 20 files | 70.6% | 100% | 55% | 0% | markdown (64.3%) |
| Project | 20 files | 58.8% | 95% | 60% | 0% | yaml (64.4%) |
| Function | 8 funcs | 39.1% | 100% | - | - | - |
| Behavioral | 8 funcs | 85.7% | - | - | 6/7 passed (1 skipped) |

## Artifacts

| Artifact | File | Size | ~Tokens | Description |
|---|---|---:|---:|---|
| Format benchmark | [`benchmark_format.json`](benchmark_format.json) | 79.7 KB | 20,402 | Format comparison across multiple files |
| Function-logic format benchmark | [`benchmark_function_logic.json`](benchmark_function_logic.json) | 16.8 KB | 4,289 | Standalone format benchmark for function-logic TOON (function.toon) |
| Token benchmark | [`benchmark_token.json`](benchmark_token.json) | 79.7 KB | 20,394 | Token efficiency comparison |
| Project benchmark | [`benchmark_project.json`](benchmark_project.json) | 80.0 KB | 20,476 | Project-level benchmark |
| Function benchmark | [`benchmark_function.json`](benchmark_function.json) | 8.2 KB | 2,099 | Function-level benchmark |
| Behavioral benchmark | [`benchmark_behavioral.json`](benchmark_behavioral.json) | 4.2 KB | 1,073 | Behavioral equivalence tests (original vs reproduced) |
| Self-analysis: TOON | [`project.toon`](project.toon) | 67.7 KB | 17,338 | Project TOON output |
| Self-analysis: function-logic TOON | [`function.toon`](function.toon) | 111.4 KB | 28,520 | Function-logic (TOON; generated with --function-logic function.toon --with-schema --compact --no-repeat-module) |
| Schema: TOON | [`project.toon-schema.json`](project.toon-schema.json) | 1.6 KB | 412 | JSON Schema for project.toon |
| Schema: function-logic | [`function-schema.json`](function-schema.json) | 2.9 KB | 749 | JSON Schema for function.toon |
| Self-analysis: YAML | [`project.yaml`](project.yaml) | 254.3 KB | 65,109 | YAML compact output |
| Self-analysis: JSON | [`project.json`](project.json) | 396.9 KB | 101,617 | JSON output |
| Self-analysis: Markdown | [`project.md`](project.md) | 139.6 KB | 35,740 | Markdown output |
| Self-analysis: Compact | [`project.txt`](project.txt) | 26.2 KB | 6,697 | Compact text output |
| Self-analysis: CSV | [`project.csv`](project.csv) | 307.9 KB | 78,825 | CSV standard output |

## Commands used

```bash
# Auto-generated by make benchmark
set -euo pipefail
poetry run python examples/15_unified_benchmark.py  --type format --folder tests/samples/ --formats yaml toon logicml json markdown csv gherkin function.toon --limit 20 --verbose --output examples/output/benchmark_format.json
poetry run python examples/15_unified_benchmark.py  --type function --file tests/samples/sample_functions.py --limit 10 --verbose --output examples/output/benchmark_function.json
poetry run python examples/behavioral_benchmark.py
poetry run python examples/11_token_benchmark.py  --folder tests/samples/ --formats yaml toon logicml json markdown csv gherkin function.toon --limit 20 --verbose --output examples/output/benchmark_token.json
poetry run python examples/15_unified_benchmark.py  --type project --folder tests/samples/ --formats yaml toon logicml json markdown csv gherkin function.toon --limit 20 --verbose --output examples/output/benchmark_project.json
poetry run python -m code2logic ./ -f toon --compact --name project -o ./
poetry run python -m code2logic ./ -f toon --compact --no-repeat-module --function-logic function.toon --with-schema --name project -o ./
poetry run python -m code2logic ./ -f yaml --compact --name project -o examples/output/
poetry run python -m code2logic ./ -f json --name project -o examples/output/
poetry run python -m code2logic ./ -f markdown --name project -o examples/output/
poetry run python -m code2logic ./ -f compact --name project -o examples/output/
poetry run python -m code2logic ./ -f csv -d standard --name project -o examples/output/
```

## Notes on score correctness

- The benchmark **does not prove functional equivalence**. It is a weighted heuristic based on:
  - Text similarity (SequenceMatcher / token overlap)
  - Structural heuristics (counts of classes/functions/imports/attributes)
  - Semantic heuristics (identifier overlap, signature/decorator presence, type hints, docstrings)
- In `--no-llm` template mode the reproduced code is a placeholder skeleton, so scores reflect spec extractability rather than true code regeneration.
- The behavioral benchmark will **skip** functions that look like template stubs (e.g. `return None`). Run with an LLM-enabled function reproduction to measure behavioral equivalence.
